

\documentclass[a4paper, 12pt]{article}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}

\usepackage[margin = 1in]{geometry}
\usepackage{color}
\begin{document}

\section*{Chapter 1} %the star prohibited the automatic numbering
\subsection*{6/16/2017}

1. Supervised learning: having the presence of outcome variable to guide the learning process. Unsupervised learning:have no measurement of the outcome, our task is clustering.

2. Classification problem(predict categorical output)and regression problem(predict quantitative output)are Supervised learning;while clustering is Unsupervised learning.

\section*{Chapter 2}
\subsection*{6/18/2017}

1. Linear model  makes huge assumption, yield stable but probably inaccurate predictions(low variance and high bias); while K-nearest neighbors model make mild assumptions, its predictions often accurate but unstable

2. \textcolor{red}{ P12,  1st paragraph, what is affine set? 	2nd paragraph, why steepest uphill direction?}

\textbf{Affine set} A subspace $S$ is a line or a plane go through the origin, i.e., $0\in S$. A subspace can be represented by a linear combination of basis vectors $b_k$'s
$$S = \{\lambda_1 b_1+\cdots+\lambda_p b_p, \lambda_k\in R\}$$.
If we move this subspace by an offset $o\notin S$ (or in the direction of $o$), we will get an affine set
$$A = o+S = \{o+\lambda_1 b_1+\cdots+\lambda_p b_p, \lambda_k\in R\}$$.

For what is discussed in the book, consider the simplest example with only one predictor
$$
y = \beta_0 + \beta_1 x
$$
\begin{itemize}
	\item If the constant is included, then each point is $z = (1,x,y)^T$. Let $\gamma = (\beta_0, \beta_1, -1)$. Then the point $z$ is in the subspace defined by $S = \{z: z^T\gamma = 0\}$.
	\item  If the constant is not included, then each point is $z = (x,y)$. Let $\gamma = (\beta_1, -1)$. Each point is in the affine set defined by $A = (0, \beta_0)^T + \{
	z: z^T\gamma = 0\}$. The offset is $o=(0, \beta_0)^T$.
\end{itemize}

\vspace{10pt}
\noindent \textbf{Greatest uphill direction}. By first order Tylor expansion,
$$
f(X+\Delta) - f(X) \approx \langle f'(X) , \Delta \rangle = \Vert f'(x)\Vert \Vert \Delta\Vert \cos\theta
$$
where $\langle \va,\vb\rangle = \sum_i a_i b_i$ is the inner product of vector 
$\va$ and $\vb$. Inner product is equal to the vector norm mutiplied by $\cos\theta$, and $\theta$ is the angle between the two vectors. If $\Delta$ is in the same direction as $f'(x)=\beta$, then $\theta = 0$ and $\cos\theta = 1$. The increase of function value $f(X+\Delta) - f(x) $ is greatest.  

Image we are climbing a mountain, our current position is $X$ and the height is $f(X)$. We wish to go to the top of the mountain. 
If we walk from $X$ in the direction of $\beta$, we get greatest increase in $f(X)$ (uphill direction).

3. The error on the training dataset should be approximately an increasing function of k for k-nearest neighbor fit; cannot use sum of squared error as a criteria for picking k.

4. The effective number of parameters in k-nearest neighbor is N/k, i.e., if non-overlapping, we need fit N/k means for totally N/k neighborhoods

5. If the training data is simulated from independent Gaussian distributions, least square is appropriate for prediction; if from mixture Gaussian, k-nearest neighbor is better.
\subsection*{6/19/2017}

1. Bullets on P17,P18: kernel methods, local regression ... enhanced the simple 1(k)-nearest neighbor procedure. Kernel methods having weights decrease smoothly to zero with distance from the target point, rather than the 0/1 weights used by k-nearest neighbors.

2. Minimize the squared prediction error(EPE) loss, $EPE(f)= E_xE_{Y|X}([Y-f(X)]^2|X)$, get solution $f(x)= E(Y|X=x)$, i.e., the conditional expectation(known as the regression function). Nearest-neighbor approximates $\hat{f}(x)=Ave(y_i|x_i \in N_k(x))$. Under mild condition on P(X,Y), as N,k $\rightarrow \infty$ such that k/N $\rightarrow$ 0, $\hat{f}(x) \rightarrow E(Y|X=x)$. The convergence still holds but the rate of convergence decreases as the dimension p increases.

3. Additive model solves the high dimensional problem by approximating the universal conditional expectation simutaneously for each of the coordinate function $f_j$ in $f(X)=\sum_{j=1}^pf_j(X)$.

4. If $L_1$ loss: $E|Y-f(X)|$, the solution will be conditional median $f(x)= median(Y|X=x)$. The discontinuity in its derivatives hendered their wide use.

5. Loss function for categorical prediction is represented by a matrix L=card(G) with 0 on the diagonal and nonnegative elsewhere(often 1s).
\textcolor{red}{ How to calculate (2.19) - (2.23)?}
\vspace{6pt}
\noindent \textbf{Equation 2.19--2.23}. 
(2.19) takes expectation with respect to both $X,\ G$. 
$$
\mathrm{EPE} = E_{X,G}[L(G,\hat{G}(X)]
$$
(2.20) uses the following relation
$$
\mathrm{EPE} = E_X\Big[ E_G[L(G,\hat{G}(X) | X]\Big]
$$
The conditional expection inside is replace by the summation in
(2.20). Then (2.21) tries to minimize the above risk to get an
estimator $\hat{G}$. (2.22) and (2.23) are the solution for 0-1 loss
function. It classifies $X$ to the most probable class. 

\subsection*{6/20/2017}

1. Curse of dimensionality: (1) To capture a fraction r of a p-dimensional hypercube,  the expected edge length will be $r^{1/p}$, i.e., to capture $1\%$ of the data forming a local hypercube, we need to cover $63\%$ of the range in each dimension/variable of the input; 

(2)Mean distance of the closest point increases for a large $p$ so the prediction is more difficult near the edges of the training sample;

(3)Sampling density is proportional to $N^{1/p}$, i.e., $100^10$ sample needed for p=10 to get same sample density of 100 in 1D.

(4)The complexity of function(Y=f(X)) with many variables(larger p) can grow with the dimension p, and if we want to estimate such functions with same accuracy as in low dim, then we need the size of training data grow exponentially.

2.Calculation in (2.27) shows why prediction error have additional variance $\sigma^2$.

3. If we assume it's a linear model, (2.28) shows the expected EPE increase linearly in p. By imposing linear restrictions, we have avoided the curse of dimensionality.

\subsection*{6/21/2017}

1. \textcolor{red}{ how to get the $\hat{y}_0$ on the bottom of page 24?}

\vspace{6pt}
\noindent \textbf{Equation 2.26, ($\hat{y}_0$)}. 
\begin{eqnarray*}
	\hat{y}_0 &=& x_0^T\hat{\beta} = x_0^T (X^TX)^{-1} X^Ty\\
	& =& x_0^T (X^TX)^{-1} X^T (X\beta + \epsilon) \\
	& = & x_0^T (X^TX)^{-1} X^T X\beta +x_0 (X^TX)^{-1} X^T X \epsilon \\
	& = & x_0^T \beta +x_0 (X^TX)^{-1} X^T X \epsilon 
\end{eqnarray*}

2. figure 2.9 shows comparison of least square and 1-nearest neighbor with their ration for two senario(get the way of comparison). There is acutally a whole spectrum of methods between the rigid linear models and the extreme flexible 1-nearest neighbor models. Section 2.6 using other f(X), specifically designed to overcome the dimensionality problem.

3. In additive error model,$Y=f(X)+\epsilon$,$\epsilon$ is independent of X.More often, (X,Y) will not have a deterministic relationship $Y=f(X)$, so the additive model assume that we can capture all the departures from a deterministic relationship via the error $\epsilon$.

4. \textcolor{red}{ 
In the beginning of 2.6.1, Why the conditional distribution  $P(Y|X)$ depends on $X$ only through the conditional mean $f(x)=E(Y|X=x)$? Answered in equation (2.34).
}

\vspace{6pt}
\noindent \textbf{Section 2.6.1}
$Y = f(X) + \epsilon$. Only $\epsilon$ is random and its distribution
is known or fixed. The $f(X)$ shifts the mean of $\epsilon$. 
Given $X$, we know $f(X)$, so we also know the distribution of $Y$. 

5. Two ways of understanding: Supervised learning VS function approximation. Learning by example: the learning algorithm modify its input/output relationship in response to the prediction error; In terms of function approximation, we imagine our parametrized function as a surface in p+1 dim space and we observed noisy representations from it.

6. The principle of MLE assumes that the most reasonable values for $\theta$ are those for which the probability of the observed sample is largest.

\section*{Chapter 3}

\subsection*{6/22/2017(Ch2 P32-P34)}

1. need Approaches to make efficient use of structured data. There are infinitely many solutions for $\hat{f}$, so we impose restritions on eligible solutions.

2. Most training methods imposed complexity restrictions in small neighborhood of input space, e.g., $\hat{f}$ exhibits special structure such as nearly constant, linear or low order polynomial behavior in the small neighborhood.

3. Any method that attempt to produce locally varying functions in small isotropic neighborhood will run into problems in high dimension; all methods that overcome the dimensionality problems have an associated metric for measuring neighborhoods, which does not allow the neighborhood to be simutaneously small in all directions.

4. nonparametric methods falls in several classes and each of the classes has associated with one or more smoothing parameters that control the effective size of the local neighborhood.

5. \textcolor{red}{ 
why called adaptively chosen basis function methods in the last and second last paragraph of page 36?
}

\subsection*{6/23/2017(Ch2 P34-P38)}

1. Roughness penalty controls the second derivative of $f$, to get a smooth solution. Penalized least-sqaure criterion $$PRSS(f;\lambda)= \sum_{i=1}^N(y_i -f(x_i))^2 + \lambda\int[f''(x)]^2dx$$. when $\lambda=\infty$, only linear function(the most smooth) are permitted. 

2. Penalty function, or regularization methods express our prior belief that the type of functions we seek exhibit smooth behavior.

3. Kernal methods specified the nature of the local neighborhood. Kernal function $K_{\lambda}(x_0,x)$ assigns weights to points x in a region around $x_0$. Define a local regression estimate $f_{\hat{\theta}}(x_0)$ where $\hat{\theta}$ minimizes $$RSS(f_{\theta},x_0)=\sum_{i=1}^NK_{\lambda}(x_0,x_i)(y-f_{\theta}(x_i))^2$$

4. Basis functions: the model $f$ is a linear expansion of basis functions $$f_{\theta}(x)=\sum_{m=1}^M\theta_mh_m(x)$$
The term linear refers to the action of parameter $\theta$.

5.  \textcolor{red}{ 
Didn't understand the 2nd paragraph on Page36, reread it after reading Ch5. Spline basis has knots. In general, we would like the data to dictate them as well but that will lead to a hard nonlinear problem.
}

6. All the above models have a smoothing or complexity parameter: The multiplier of the penalty term; the width of the kernel; the number of basis functions. We cannot use residual sum-of-squares on the training data to determine these parameters since it will always pick the interpolation fits hence zero residuals.

7. As the model complexity increases, the variance tends to be increase and bias decreases. k-nearest neighbor as k increase, variance decrease and squared bias increases.

8. Figure 2.11 shows the typical behavior of test error and training error: as model complexity increase, the training error would decrease since the model adapts itself too closely to the training data(overfitting) and will not generalize well(large test error).

\subsection*{6/24/2017(Ch3 P43-P46,Notes made up on 6/25)}

1. Linear model assumes that the regression function $E(Y|X)$ is linear \textcolor{red}{  in X not in $\beta$! } Linear models can be applied to transformations(like the basis) of the input, which considerably expand their scope.

2. Even if the $x_i$ were not drawn randomly, the least square criterion is still valid if the $y_i$'s are conditionally independent given the input $x_i$.\textcolor{red}{ (Did not assume normality in LSE, right?) }

3. The estimate $\hat{y}$ is the orthogonal projection of y onto the subspace spanned by x, and residual vector $y-\hat{y}$ is orthogonal to this subspace.

4. If X is not full rank,$X^TX$ is singular and the least square coefficients $\hat{\beta}$ are not uniquely defined. However, the fitted values $\hat{y}$ are still the projection of y onto the column space of X; there is just more than one way to express that projection in terms of X.

5. Rank deficiency can also occur in signal and image analysis, where $p>N$. In this case, the features are typically reduced by filtering or controlled by regularization.

\subsection*{6/25/2016 (Ch3 P46-51)}

1.$$Var(\hat{\beta})=(X^TX)^{-1}\sigma^2$$
To  draw inference about the parameters and the  model, need additional Gaussian assumption
$$\hat{\beta} \sim N(\beta,(X^TX)^{-1}\sigma^2 )$$
$$(N-p-1)\hat(\sigma^2) \sim \sigma^2\chi^2_{N-p-1}$$
$\hat{\beta}$ and $\hat{\sigma}^2$ are statistically independent.

2. Equation 3.13, the F statistics measures the changes in residual sum-of-squares per additional parameter in the bigger model, and it is normalized by an estimate of $\sigma^2$. The F statistics for dropping a single coefficient from a model is equal to the square of the corresponding z-score(3.12), since $t_{\nu}^2=F_{1,\nu}$

3. Even if the Gaussian error assumption not hold, the CI will be approximately correct with its coverage approaching $1-2\alpha$ as sample size $N \rightarrow \infty$.

4. calculate z-score for all $\hat{\beta}$'s, a z-score greater than 2 in absolute value is approximately significant at the $5\%$ level.

\subsection*{6/26/2016 (Ch3 P51-P55)}

1. The LSE of parameter $\beta$ have the smallest variance among all linear unbiased estimates. Restricted to unbiased estimates is not necessary wise. From a pragmatic point of view, most models are distortion of the truth and hence are biased; picking models amounts to creating the right balance btw bias and variance.

2. The multiple regression coefficient $\hat{\beta}_j$ represents the additional contribution of $x_j$ on y, after $x_j$ has been adjusted for $x_0,x_1,\ldots,x_{j-1},x_{j+1},\ldots,x_p$.

3. For univariate model, $\hat{\beta}=\frac{<x,y>}{<x,x>}$. Suppose that columns of X are orthogonal, them the multiple least square estimates $\beta_j=\frac{<x_j,y>}{<x_j,x_j>}$ the univariate estimates, i.e., when inputs are orthogonal, they have no effect on each other's parameter estimates. Algorithm 3.1 Page 54 is known as Gram-Schmidt procedure for multiple regression(orthogonalized the inputs X):
\begin{itemize}
	\item $z_0 =\textbf{1}$, regress $x_1$ on $z_0$, $x_1=z_0\gamma_{01} + e$, get residual $z_1=x_1-z_0\hat{\gamma}_{01}$, regress y on $z_1$ get $\hat{\beta}_1$
	\item regress $x_2$ on $z_1,z_0$, get residual $z_2=x_2-z_1\hat{\gamma}_{12}-z_0\hat{\gamma}_{02}$, regress y on $z_2$ get $\hat{\beta}_2$
	\item $\ldots$
\end{itemize}


\subsection*{6/27/2016 (Ch3 P55-P58)}

1. We can represent step 2 of Algorithm 3.1 in matrix form: $X=Z\Gamma$, where z has columns $z_j$ in order and $\Gamma$ is the upper triangular matrix with entries$\hat{\gamma}_{k,j}$. Diagonal matrix D with j-th diagonal entry $D_{jj}=||z_j||^2$ to standardize Z in $X=ZD^{-1}D\Gamma=QR$.$Q^TQ=I$. QR decomposition represents a convenient orthogonal basis for the column space of X. Easy to get(with R being upper triangular matrix).
$$\hat{\beta}= R^{-1}Q^Ty; \hat{y}=QQ^Ty$$

2. For multivariate output Y, $RSS(B)=tr((Y-XB)^T(Y-XB))$, $\hat{B}$ same format of LSE as for univariate y \textcolor{red}{even if $cov(\epsilon)=\Sigma$, why? (not the case if $\Sigma_i$ differ for observations i)}.

\end{document}


\documentclass[a4paper, 12pt]{article}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}

\usepackage[margin = 1in]{geometry}
\usepackage{color}
\usepackage{amsmath}
\begin{document}

\section*{Chapter 1} %the star prohibited the automatic numbering
\subsection*{6/16/2017}

1. Supervised learning: having the presence of outcome variable to guide the learning process. Unsupervised learning:have no measurement of the outcome, our task is clustering.

2. Classification problem(predict categorical output)and regression problem(predict quantitative output)are Supervised learning;while clustering is Unsupervised learning.

\section*{Chapter 2}
\subsection*{6/18/2017}

1. Linear model  makes huge assumption, yield stable but probably inaccurate predictions(low variance and high bias); while K-nearest neighbors model make mild assumptions, its predictions often accurate but unstable

2. \textcolor{red}{ P12,  1st paragraph, what is affine set? 	2nd paragraph, why steepest uphill direction?}

\textbf{Affine set} A subspace $S$ is a line or a plane go through the origin, i.e., $0\in S$. A subspace can be represented by a linear combination of basis vectors $b_k$'s
$$S = \{\lambda_1 b_1+\cdots+\lambda_p b_p, \lambda_k\in R\}$$.
If we move this subspace by an offset $o\notin S$ (or in the direction of $o$), we will get an affine set
$$A = o+S = \{o+\lambda_1 b_1+\cdots+\lambda_p b_p, \lambda_k\in R\}$$.

For what is discussed in the book, consider the simplest example with only one predictor
$$
y = \beta_0 + \beta_1 x
$$
\begin{itemize}
	\item If the constant is included, then each point is $z = (1,x,y)^T$. Let $\gamma = (\beta_0, \beta_1, -1)$. Then the point $z$ is in the subspace defined by $S = \{z: z^T\gamma = 0\}$.
	\item  If the constant is not included, then each point is $z = (x,y)$. Let $\gamma = (\beta_1, -1)$. Each point is in the affine set defined by $A = (0, \beta_0)^T + \{
	z: z^T\gamma = 0\}$. The offset is $o=(0, \beta_0)^T$.
\end{itemize}

\vspace{10pt}
\noindent \textbf{Greatest uphill direction}. By first order Tylor expansion,
$$
f(X+\Delta) - f(X) \approx \langle f'(X) , \Delta \rangle = \Vert f'(x)\Vert \Vert \Delta\Vert \cos\theta
$$
where $\langle \va,\vb\rangle = \sum_i a_i b_i$ is the inner product of vector 
$\va$ and $\vb$. Inner product is equal to the vector norm mutiplied by $\cos\theta$, and $\theta$ is the angle between the two vectors. If $\Delta$ is in the same direction as $f'(x)=\beta$, then $\theta = 0$ and $\cos\theta = 1$. The increase of function value $f(X+\Delta) - f(x) $ is greatest.  

Image we are climbing a mountain, our current position is $X$ and the height is $f(X)$. We wish to go to the top of the mountain. 
If we walk from $X$ in the direction of $\beta$, we get greatest increase in $f(X)$ (uphill direction).

3. The error on the training dataset should be approximately an increasing function of k for k-nearest neighbor fit; cannot use sum of squared error as a criteria for picking k.

4. The effective number of parameters in k-nearest neighbor is N/k, i.e., if non-overlapping, we need fit N/k means for totally N/k neighborhoods

5. If the training data is simulated from independent Gaussian distributions, least square is appropriate for prediction; if from mixture Gaussian, k-nearest neighbor is better.
\subsection*{6/19/2017}

1. Bullets on P17,P18: kernel methods, local regression ... enhanced the simple 1(k)-nearest neighbor procedure. Kernel methods having weights decrease smoothly to zero with distance from the target point, rather than the 0/1 weights used by k-nearest neighbors.

2. Minimize the squared prediction error(EPE) loss, $EPE(f)= E_xE_{Y|X}([Y-f(X)]^2|X)$, get solution $f(x)= E(Y|X=x)$, i.e., the conditional expectation(known as the regression function). Nearest-neighbor approximates $\hat{f}(x)=Ave(y_i|x_i \in N_k(x))$. Under mild condition on P(X,Y), as N,k $\rightarrow \infty$ such that k/N $\rightarrow$ 0, $\hat{f}(x) \rightarrow E(Y|X=x)$. The convergence still holds but the rate of convergence decreases as the dimension p increases.

3. Additive model solves the high dimensional problem by approximating the universal conditional expectation simutaneously for each of the coordinate function $f_j$ in $f(X)=\sum_{j=1}^pf_j(X)$.

4. If $L_1$ loss: $E|Y-f(X)|$, the solution will be conditional median $f(x)= median(Y|X=x)$. The discontinuity in its derivatives hendered their wide use.

5. Loss function for categorical prediction is represented by a matrix L=card(G) with 0 on the diagonal and nonnegative elsewhere(often 1s).
\textcolor{red}{ How to calculate (2.19) - (2.23)?}
\vspace{6pt}
\noindent \textbf{Equation 2.19--2.23}. 
(2.19) takes expectation with respect to both $X,\ G$. 
$$
\mathrm{EPE} = E_{X,G}[L(G,\hat{G}(X)]
$$
(2.20) uses the following relation
$$
\mathrm{EPE} = E_X\Big[ E_G[L(G,\hat{G}(X) | X]\Big]
$$
The conditional expection inside is replace by the summation in
(2.20). Then (2.21) tries to minimize the above risk to get an
estimator $\hat{G}$. (2.22) and (2.23) are the solution for 0-1 loss
function. It classifies $X$ to the most probable class. 

\subsection*{6/20/2017}

1. Curse of dimensionality: (1) To capture a fraction r of a p-dimensional hypercube,  the expected edge length will be $r^{1/p}$, i.e., to capture $1\%$ of the data forming a local hypercube, we need to cover $63\%$ of the range in each dimension/variable of the input; 

(2)Mean distance of the closest point increases for a large $p$ so the prediction is more difficult near the edges of the training sample;

(3)Sampling density is proportional to $N^{1/p}$, i.e., $100^10$ sample needed for p=10 to get same sample density of 100 in 1D.

(4)The complexity of function(Y=f(X)) with many variables(larger p) can grow with the dimension p, and if we want to estimate such functions with same accuracy as in low dim, then we need the size of training data grow exponentially.

2.Calculation in (2.27) shows why prediction error have additional variance $\sigma^2$.

3. If we assume it's a linear model, (2.28) shows the expected EPE increase linearly in p. By imposing linear restrictions, we have avoided the curse of dimensionality.

\subsection*{6/21/2017}

1. \textcolor{red}{ how to get the $\hat{y}_0$ on the bottom of page 24?}

\vspace{6pt}
\noindent \textbf{Equation 2.26, ($\hat{y}_0$)}. 
\begin{eqnarray*}
	\hat{y}_0 &=& x_0^T\hat{\beta} = x_0^T (X^TX)^{-1} X^Ty\\
	& =& x_0^T (X^TX)^{-1} X^T (X\beta + \epsilon) \\
	& = & x_0^T (X^TX)^{-1} X^T X\beta +x_0 (X^TX)^{-1} X^T X \epsilon \\
	& = & x_0^T \beta +x_0 (X^TX)^{-1} X^T X \epsilon 
\end{eqnarray*}

2. figure 2.9 shows comparison of least square and 1-nearest neighbor with their ration for two senario(get the way of comparison). There is acutally a whole spectrum of methods between the rigid linear models and the extreme flexible 1-nearest neighbor models. Section 2.6 using other f(X), specifically designed to overcome the dimensionality problem.

3. In additive error model,$Y=f(X)+\epsilon$,$\epsilon$ is independent of X.More often, (X,Y) will not have a deterministic relationship $Y=f(X)$, so the additive model assume that we can capture all the departures from a deterministic relationship via the error $\epsilon$.

4. \textcolor{red}{ 
In the beginning of 2.6.1, Why the conditional distribution  $P(Y|X)$ depends on $X$ only through the conditional mean $f(x)=E(Y|X=x)$? Answered in equation (2.34).
}

\vspace{6pt}
\noindent \textbf{Section 2.6.1}
$Y = f(X) + \epsilon$. Only $\epsilon$ is random and its distribution
is known or fixed. The $f(X)$ shifts the mean of $\epsilon$. 
Given $X$, we know $f(X)$, so we also know the distribution of $Y$. 

5. Two ways of understanding: Supervised learning VS function approximation. Learning by example: the learning algorithm modify its input/output relationship in response to the prediction error; In terms of function approximation, we imagine our parametrized function as a surface in p+1 dim space and we observed noisy representations from it.

6. The principle of MLE assumes that the most reasonable values for $\theta$ are those for which the probability of the observed sample is largest.

\section*{Chapter 3}

\subsection*{6/22/2017(Ch2 P32-P34)}

1. need Approaches to make efficient use of structured data. There are infinitely many solutions for $\hat{f}$, so we impose restritions on eligible solutions.

2. Most training methods imposed complexity restrictions in small neighborhood of input space, e.g., $\hat{f}$ exhibits special structure such as nearly constant, linear or low order polynomial behavior in the small neighborhood.

3. Any method that attempt to produce locally varying functions in small isotropic neighborhood will run into problems in high dimension; all methods that overcome the dimensionality problems have an associated metric for measuring neighborhoods, which does not allow the neighborhood to be simutaneously small in all directions.

4. nonparametric methods falls in several classes and each of the classes has associated with one or more smoothing parameters that control the effective size of the local neighborhood.

5. \textcolor{red}{ 
why called adaptively chosen basis function methods in the last and second last paragraph of page 36?
}

\subsection*{6/23/2017(Ch2 P34-P38)}

1. Roughness penalty controls the second derivative of $f$, to get a smooth solution. Penalized least-sqaure criterion $$PRSS(f;\lambda)= \sum_{i=1}^N(y_i -f(x_i))^2 + \lambda\int[f''(x)]^2dx$$. when $\lambda=\infty$, only linear function(the most smooth) are permitted. 

2. Penalty function, or regularization methods express our prior belief that the type of functions we seek exhibit smooth behavior.

3. Kernal methods specified the nature of the local neighborhood. Kernal function $K_{\lambda}(x_0,x)$ assigns weights to points x in a region around $x_0$. Define a local regression estimate $f_{\hat{\theta}}(x_0)$ where $\hat{\theta}$ minimizes $$RSS(f_{\theta},x_0)=\sum_{i=1}^NK_{\lambda}(x_0,x_i)(y-f_{\theta}(x_i))^2$$

4. Basis functions: the model $f$ is a linear expansion of basis functions $$f_{\theta}(x)=\sum_{m=1}^M\theta_mh_m(x)$$
The term linear refers to the action of parameter $\theta$.

5.  \textcolor{red}{ 
Didn't understand the 2nd paragraph on Page36, reread it after reading Ch5. Spline basis has knots. In general, we would like the data to dictate them as well but that will lead to a hard nonlinear problem.
}

6. All the above models have a smoothing or complexity parameter: The multiplier of the penalty term; the width of the kernel; the number of basis functions. We cannot use residual sum-of-squares on the training data to determine these parameters since it will always pick the interpolation fits hence zero residuals.

7. As the model complexity increases, the variance tends to be increase and bias decreases. k-nearest neighbor as k increase, variance decrease and squared bias increases.

8. Figure 2.11 shows the typical behavior of test error and training error: as model complexity increase, the training error would decrease since the model adapts itself too closely to the training data(overfitting) and will not generalize well(large test error).

\subsection*{6/24/2017(Ch3 P43-P46,Notes made up on 6/25)}

1. Linear model assumes that the regression function $E(Y|X)$ is linear \textcolor{red}{  in X not in $\beta$! } Linear models can be applied to transformations(like the basis) of the input, which considerably expand their scope.

2. Even if the $x_i$ were not drawn randomly, the least square criterion is still valid if the $y_i$'s are conditionally independent given the input $x_i$.\textcolor{red}{ (Did not assume normality in LSE, right?) }

3. The estimate $\hat{y}$ is the orthogonal projection of y onto the subspace spanned by x, and residual vector $y-\hat{y}$ is orthogonal to this subspace.

4. If X is not full rank,$X^TX$ is singular and the least square coefficients $\hat{\beta}$ are not uniquely defined. However, the fitted values $\hat{y}$ are still the projection of y onto the column space of X; there is just more than one way to express that projection in terms of X.

5. Rank deficiency can also occur in signal and image analysis, where $p>N$. In this case, the features are typically reduced by filtering or controlled by regularization.

\subsection*{6/25/2016 (Ch3 P46-51)}

1.$$Var(\hat{\beta})=(X^TX)^{-1}\sigma^2$$
To  draw inference about the parameters and the  model, need additional Gaussian assumption
$$\hat{\beta} \sim N(\beta,(X^TX)^{-1}\sigma^2 )$$
$$(N-p-1)\hat(\sigma^2) \sim \sigma^2\chi^2_{N-p-1}$$
$\hat{\beta}$ and $\hat{\sigma}^2$ are statistically independent.

2. Equation 3.13, the F statistics measures the changes in residual sum-of-squares per additional parameter in the bigger model, and it is normalized by an estimate of $\sigma^2$. The F statistics for dropping a single coefficient from a model is equal to the square of the corresponding z-score(3.12), since $t_{\nu}^2=F_{1,\nu}$

3. Even if the Gaussian error assumption not hold, the CI will be approximately correct with its coverage approaching $1-2\alpha$ as sample size $N \rightarrow \infty$.

4. calculate z-score for all $\hat{\beta}$'s, a z-score greater than 2 in absolute value is approximately significant at the $5\%$ level.

\subsection*{6/26/2016 (Ch3 P51-P55)}

1. The LSE of parameter $\beta$ have the smallest variance among all linear unbiased estimates. Restricted to unbiased estimates is not necessary wise. From a pragmatic point of view, most models are distortion of the truth and hence are biased; picking models amounts to creating the right balance btw bias and variance.

2. The multiple regression coefficient $\hat{\beta}_j$ represents the additional contribution of $x_j$ on y, after $x_j$ has been adjusted for $x_0,x_1,\ldots,x_{j-1},x_{j+1},\ldots,x_p$.

3. For univariate model, $\hat{\beta}=\frac{<x,y>}{<x,x>}$. Suppose that columns of X are orthogonal, them the multiple least square estimates $\beta_j=\frac{<x_j,y>}{<x_j,x_j>}$ the univariate estimates, i.e., when inputs are orthogonal, they have no effect on each other's parameter estimates. Algorithm 3.1 Page 54 is known as Gram-Schmidt procedure for multiple regression(orthogonalized the inputs X):
\begin{itemize}
	\item $z_0 =\textbf{1}$, regress $x_1$ on $z_0$, $x_1=z_0\gamma_{01} + e$, get residual $z_1=x_1-z_0\hat{\gamma}_{01}$, regress y on $z_1$ get $\hat{\beta}_1$
	\item regress $x_2$ on $z_1,z_0$, get residual $z_2=x_2-z_1\hat{\gamma}_{12}-z_0\hat{\gamma}_{02}$, regress y on $z_2$ get $\hat{\beta}_2$
	\item $\ldots$
\end{itemize}


\subsection*{6/27/2016 (Ch3 P55-P57)}

1. We can represent step 2 of Algorithm 3.1 in matrix form: $X=Z\Gamma$, where z has columns $z_j$ in order and $\Gamma$ is the upper triangular matrix with entries$\hat{\gamma}_{k,j}$. Diagonal matrix D with j-th diagonal entry $D_{jj}=||z_j||^2$ to standardize Z in $X=ZD^{-1}D\Gamma=QR$.$Q^TQ=I$. QR decomposition represents a convenient orthogonal basis for the column space of X. Easy to get(with R being upper triangular matrix).
$$\hat{\beta}= R^{-1}Q^Ty; \hat{y}=QQ^Ty$$

2. For multivariate output Y, $RSS(B)=tr((Y-XB)^T(Y-XB))$, $\hat{B}$ same format of LSE as for univariate y \textcolor{red}{even if $cov(\epsilon)=\Sigma$, why? (not the case if $\Sigma_i$ differ for observations i)}.

\subsection*{6/28/2016 (Ch3 P57-P62)}

1. Backward selection can only be used when $N>p$, while forward selection can always be used. Hybrid stepwise selection consider both forward and backward moves at each step, and select the "best" of the two.

2.  forward-stagewise regression starts with an intercept equals to $\overline{y}$ and centered predictors with coefficients initially at 0. At each step, it identifies the variable most correlated with the residual and then compute the SLR coefficient on the chosen variable and adds it to the current coefficient for that variable. It will continue until none of variables have correlation with residuals. Slow but competitive for high-dim problems.

3. description of 10 fold cross validation 3.3.4 Page61 1st paragraph. 

4. Model selection is a discrete process and often exhibit high variance. Shrinkage methods are more continuous and don't suffer as much from high variability.

\subsection*{6/29/2016 (Ch3 P63-P67)}

1. The main motivation for ridge regression when first introduced is to make $X^TX$ nonsingular by adding a $\lambda$. In the case of orthogonal inputs X, the ridge regression estimate $\beta_{ridge}=\hat{\beta}/(1+\lambda)$. The ridge estimate is the mode or mean of the posterior distribution of $\beta$, with $\lambda=\sigma^2/\tau^2$, prior $\beta \sim N(0,\tau^2)$.

2. \textcolor{red}{What conclusion can be draw from the profile plot of ridge regression Page64 Figure 3.8}

3. Page 66 connect ridge regression,SVD, LSE and principle component, very helpful!!! Ridge regression shrinks the coordinates by factor of $d^2_j/(d^2_j+\lambda)$, which means a greater shrinkage is applied to the coordinate of basis vectors with smaller $d^2_j$, which corresponding to smallest principle component direction, i.e., the direction in the column space of X having smallest variance. The small singular value corresponding to the direction in the column space of X having smallest variance.

\subsection*{6/30/2016 (Ch3 P67-P70)}

1. The implicit assumption for PCA or ridge regression is that the response will tend to vary most in the directions of high variance of the input, which is often reasonable since predictors are often chosen for study because they vary with the response variable.

2. effective df of ridge reg: $df(\lambda)=\sum_{j=1}^{p}\frac{d^2_j}{d^2_j+\lambda}$. $df(\lambda)=p$ when $\lambda=0$,$df(\lambda)=0$ when $\lambda=\infty$

3. $L_2$ ridge penalty $\sum_{1}^{p}\beta^2_j$ is replaced by $L_1$ penalty $\sum_{1}^{p}|\beta_j|$, which makes the lasso solution nonlinear in $y$ and having no explicit expression.

4. The standardized tuning parameter $s=t/\sum_{1}^{p}|\hat{\beta}^{LS}_j|$ indicate the percentage that LSE coefficients are shrunk on average. At $s=1$, lasso gives LSE; and the estimates decrease to 0 as $s \rightarrow 0$.

5. profile plot for Lasso is piece-wise linear(less computation) and it hits 0 while ridge does not.

\subsection*{7/1/2017 (Ch3 P69-P73)}

1. Lasso is "soft-thresholding"; Best-subset selection is a form of "hard-thresholding".

2. Figures on Page 71 is useful!

3. We can generalize ridge regression and the lasso and view then as Bayes estimates.
$$\tilde{\beta}=argmin{\sum_{i=1}^{N}(y_i-\beta_0-\sum_{j=1}^{p}x_{ij}\beta_j)^2 +\lambda\sum_{j=1}^{p}|\beta_j|^q}$$
q=1 corresponding to lasso with double exponential or Laplace (log)prior for $\beta_j$ with density$(1/2\tau exp(-|\beta|/\tau))$ and $\tau=1/\lambda$; q=2 to ridge regression. Ridge regression is the posterior mean but lasso and best-subset are not.

4. q=1(lasso) is the smallest q such that the constraint region is convex.

5. with $q>1$,$|\beta_j|^q$ is differentiable at 0(no corner on the constraint region), so does not share the ability of lasso for setting coefficients exactly to 0. Partially for this reason as well as for computational tractability, elastic-net is proposed, a compromise between lasso and ridge.

\subsection*{7/2/2017 (Ch3 P73-P79)}

1. LAR "least angle regression" provides an extremely efficient algorithm for computing the entire lasso path as in Figure 3.10.

2. By construction, the coefficients in LAR change in a piecewise linear fashion. Using knowledge of the covariance of the predictors and the piecewise linearity of the algorithm 3.2(LARS) on Page74, we can figure out the exact step length(like the joint point in Figure 3.14). "Least Angel Regression and Shrinkage"

3. Algorithm 3.2a(LAR) gives the entire lasso path.

4. In classical statistics, the number of linearly independent parameters is what is meant by "degree of freedom". A more general definition of the effective degrees of freedom of an adaptively fitted model:$$df(\hat{y})=\frac{1}{\sigma^2}\sum_{i=1}^{N}cov(\hat{y}_i,y_i)$$
Intuitively, the harder that we fit the data, the larger this covariance and hence $df(\hat{y})$.

\subsection*{7/3/2017 (Ch3 P79-P83)}

1. Principle components regression(PCR) forms orthogonal inputs $z_m=Xv_m$ for $m=1,\ldots,M \le p$. if M=p, we will get back the usual LSE since the columns of $Z=UD$ span the column space of X. If $M < p$ we get reduce regression like ridge: ridge regression shrinks the coef of the PCs, shrinking more depend on the size of the corresponding eigenvalues; principle components regression discards the $p-M$ smallest eigenvalue components.

2. Partial least squares(PLS) also construct a set of linear combination of the inputs but also uses $y$(unlike PCR), produces a sequence of derived, orthogonal inputs as in Algorithm 3.3. The PLS seeks directions that have high variance AND high correlation with the response, while PCR only on high variance. 

3. \textcolor{red}{ what is the rationals behind construction as $z_m=\sum_{j=1}^{p}\psi_{mj}x_j^{(m-1)} $ in PLS, for orthogonality?}

4. PLS, PCR and ridge regression behave similarly, ridge regression is preferred because it shrinks smoothly. Lasso fall between ridge and best subset, and enjoys some properties of each.

\subsection*{7/4/2017 (Ch3 P86-P93)}

1. Incremental Forward Stage-wise regression generates a coefficient profile by repeatedly updating(by a small amount $\epsilon$) the coefficient of the variable most correlated with the current residuals. Letting $\epsilon \rightarrow 0$ could give identical lasso path. \textcolor{red}{$\delta_j=<x_j,r>$(the least squares coefficients of the residual on jth predictor!!)}

2. The grouped lasso is to shrink and select the members of a group together by minimizing the convex criterion:$$\displaystyle{\min_{\beta \in R^p}}\Bigg(||y-\beta_01-\sum_{l=1}^{L}X_l\beta_l||_2^2 + \lambda\sum_{l=1}^{L}\sqrt{p_l}||\beta_l||_2\Bigg)$$

3. Caomputation of lasso via the LAR algorithm has the same order of computation as a least square fit.


\subsection*{7/5/2017 (Ch3 P101-P105)}

1. The regression approach is a member of a class of methods that model discriminant functions $\delta_k(x)$ for each class, and then classify x to the class with the largest value for its discriminant function. To get linear boundaries, all we need is that some monotone transformation of $\delta_k$ be linear, like logit transformation for logistic regression, where the decision boundary is the set of points for which the log-odds are zero.

2. Figure 4.1 shows the scenario that linear functions in the augmented space map to the quadratic function in the original space, when using with basis functions.

3. \textcolor{red}{why $\sum_{k \in \mathcal{G}}^{}\hat{f}_k(x)=1$ as long as there is intercept?Page104 2nd paragraph. $\hat{f}_k$ can be negative or larger than 1 as a consequence of the rigid linear regression}

4. \textcolor{red}{Page 104 Did not really understand after paragraph 2... till end of page 106 }

\subsection*{7/6/2017 (Ch3 P104-P109)}

1. main information extracted from 104-106: linear regression model for indicator could mask some categories like in Figure 4.2, while LDA works better.  \textcolor{red}{Did not understand Figure 4.3}, but the main idea is that for $K \ge 3$, using polynomials of order $K-1$ is good.

2. LDA arises in the special case when we assume that the classes have a common covariance matrix. Checking the log-ratio of two classes is linear in x so that the class boundary is linear.

\subsection*{7/7/2017 (Machine Learning - A probability perspective 4.2-4.2.2}

1. Gaussian discriminant analysis: define the class conditional density using multivariate Gaussian. $$p(x|y=c,\theta)=N(x|\mu_c,\Sigma_c)$$. If $\Sigma_c$ is diagonal, this is equivalent to Naive Bayes.

2. Naive Bayes classifier: Assume the features are conditionally independent given the class label. $p(x|y=c,\theta)=\prod_{j=1}^{D}p(x_j|y=c,\theta_{jc})$, D is the number of features. work well even if independent assumption not valid. Simple model with only $O(C*D)$ parameters hence relatively immune to overfitting.

3. generative classifier(Equation 2.13) specify how to generate the data using the class prior $p(y=c)$, class conditional density $p(x|y=c)$.
$$p(y=c|x)=\frac{p(y=c)p(x|y=c)}{\sum_{c'}^{}p(y=c')p(x|y=c')}$$

4. $\hat{y}(x)=\underset{c}{\mathrm{argmax}} p(y=c|x)= \underset{c}{\mathrm{argmax}} logp(y=c) + logp(x|y=c) $ The later part measures the distance from x to the center of each class, $\mu_c$, using Mahalanobis distance. For a uniform prior, $\hat{y}=\underset{c}{\mathrm{argmin}} (x-\mu_c)^T\Sigma_c^{-1}(x-\mu_c))$.

5. quadratic discriminant analysis is a typical Gaussian discriminant analysis.

6. linear discriminant analysis is for $\Sigma_c=\Sigma$, the covariance matrices are tied or shared across classes. Quadratic part canceled out between numerator and denominator. Taking log will give a linear function of x, the decision boundary is linear. $p(y=c|x)$ can be written as a softmax function: $\frac{e^{\eta_c}}{\sum_{c'=1}^{C}e^{\eta_{c'}}}$. Devide $\eta_c$ by Temperature T, at low T, the distribution stay in the most probable state, while at high T, it visits all states uniformly(Figure 4.4), like Boltzmann distribution.
	
\end{document}
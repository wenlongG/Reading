

\documentclass[a4paper, 12pt]{article}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}

\usepackage[margin = 1in]{geometry}
\usepackage{color}
\usepackage{amsmath}
\begin{document}

\section*{Linear model selection and regularization Ch6 }
\subsection{5/14/2018}
\begin{itemize}
	\item Hyperplane P338
	\item Maximal margin classifier: choose the optimal separating hyperplane, which is farthest from the training observations. The smallest perpendicular distance from all training observations to the hyperplane is called margin. Maximal margin hyperplane has the farthest minimum distance to the training observations, which represents the mid-line of the widest "slab" that we can insert between the two classes. Optimization problem (9.9)-(9.11)!!! P343
	\item Support vectors: the maximal margin hyperplane depends only on a small subset of the observations(that lies on the edge of the margin) but not on the other observations.  if these points are moved then the maximal margin hyperplane would move as well. A movement to any of the other observations would not affect the separating hyperplane, provided that the movement doesn't across the margin boundary	.
	\item Support vector classifier: when the observations are not separable by a hyperplane or when the separating hyperplane is very sensitive to a change in a single observation, we will use support vector classifier that allows some misclassified observations(on the wrong side of margin or even the incorrect side of hyperplane, so called soft margin classifier), to achieve greater robustness and better classification of most training observations.Optimization problem (9.12)-(9.15)!!! P346
	\item C is a tuning parameter, which is the budget for the amount that the margin can be violated. No more than C observations can be on the wrong side of the hyperplane. As with tuning parameters that we've seen throughout this book, C controls the bias-variance trade-off of the classifier. When C is larger, the margin is wider and allow more violations; this amounts to fitting the data less hard and obtaining a classifier more biased but have lower variance. 
	\item Only observations that lie on the margin or that violated the margin will affect the hyperplane, which are called support vectors. If C is large, then there are more support vectors (involve in determining the hyperplane), which leads to low variance of the classifier
	\item support vector classifier is applied when the boundary between two classes is linear. Performance of linear models can suffer when there is a non-linear relationship (non-linear boundaries), in which we consider enlarging the feature space using  quadratic, cubic, and even polynomial functions of the predictors to address the non-linearity. Optimization problem (9.16)!!! P350.
	\item support vector machine is an extension of support vector classifier that results from enlarging the feature space using kernels. The computation details:  the solution to support vector classifier problem(9.12)-(9.15) involves only the inner products of the observations, which can be represented as 
	\[
	f(x)=\beta_0 +\sum{i=0}{n} \alpha_i<x,x_i>
	\]
It turns out that $\alpha_i$ is nonzero only for the support vectors $x_i$. We replace the inner product with a generalization of it in kernel form
\[
K(x_i,x_{i'})
\]
A kernel is a function that quantifies the similarity of two observations. If taking 
\[
K(x_i,x_{i'})=\sum{j=1}{p}x_{ij}x_{i'j}
\]
it just give us back the linear support vector classifier so it's called a linear kernel. If taking polynomial kernel
\[
K(x_i,x_{i'})=(1+\sum{j=1}{p}x_{ij}x_{i'j})^d
\]
with d > 1, it leads to a much more flexible decision boundary. It essentially amounts to fitting a support vector classifier in a higher-dimensional space involving polynomials of degree d. When the support vector classifier is combined with a non-linear kernel, the resulting classifier is known as support vector machine.
\item redial kernel 
\[ K(x_i,x_{i'})=exp(-\gamma \sum{j=1}{p}(x_{ij}-x_{i'j})^2),
\]
where $\gamma$ is a positive constant. If a testing observation $x^*$is far from training observation $x_i$, then $x_i$ will play virtually no role in $f(x^*)$. This means that the radial kernel has very local behavior, in the sense that only nearby training observations have an effect on the class label of the test obs.
\item The advantage of using a kernel rather than simply enlarging the feature space using functions of the original features is computation efficiency. Using kernels, one need only compute K for C_n^2 distinct pairs, which can be done without explicitly working with the enlarged feature space. It's important because in many applications of SVMs, the enlarged feature space is so large that computation is intractable/unmanageable. For radial kernel, the feature space if implicit and infinite-dimensional so we could never do the computation explicitly.
\end{itemize}
	
	
\section*{Linear model selection and regularization Ch6 }
\subsection{2/11/2018}
\begin{itemize}
	\item The best subset selection, first get best models $M_0,\ldots,M_p$ for 0 to p variables in the model(each need $C_k^p$ models to compare), then use CV to compare $M_0,\ldots,M_p$ using CV test error $C_p$, AIC,BIC or $R^2$.
	\item In the case of logistic regression, instead of ordering models by RSS, we use the deviance, which is $-2*maxloglik$; the smaller the deviance, the better the fit. 
	\item The best subset selection cannot be applied with large p, not only out of the computational reason, but also because the larger the search space, the higher the chance to find models that look good on training data, thus lead to overfitting and high variance of the coefficient estimates.
	\item Forward stepwise selection begins with a model contains no predictors and then adds predictors to the model, one-at-a-time. In particular, at each step that variable that gives the greatest additional improvement to the fit is added to the model. Similarly as in best subset selection, use CV to compare $M_0,\ldots,M_p$. Best subset selection needs to compare $2^p$ models, whereas forward stepwise involves $1+\sum_{k=0}^{p-1}(p-k)=1+p(p+1)/2$ models. Forward stepwise has computational advantage over best subset but not guarantee to find the best possible model.
	\item Backward selection also search trough $1+p(p+1)/2$ models and not guaranteed to yield the best model. Backward selection requires that $n > p$(so that the full model can be fit). In contrast, forward stepwise selection can be used even when $n<p$ and it can only construct models $M_0,\ldots,M_{n-1}$ since each submodel is fit using least squares, which will not yield a unique solution if $p\ge n$
	\item Hybrid approach, stepwise selection, in analogy to froward selection but after adding each new variable, it may also remove varaibles that no longer provide an improvement in the model fit. Such an approach attempts to more closely mimic best subset selection while retaining the computational advantage of forward and backward stepwise selection.
	\item Because the training error rate will decrease as more variables are included in the model but the test error may not, training set RSS or $R^2$ cannot be used to select among a set of models with different numbers of variables. To estimate this test error rate, two common approaches:
	\begin{itemize}
		\item indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting, $C_p$, AIC, BIC, adjust $R^2$.
		\item use CV or validation set approach to directly estimate test error.
	\end{itemize}
	\item For a least squares model containing d predictors, the Cp estimate of test MSE is computed as \[C_p=\frac{1}{n}(RSS + 2d\hat{\sigma}^2)
	\]
	It can be shown that Vp is an unbiased estimate of test MSE if $\hat{\sigma}^2$ is an unbiased estimate of ${\sigma}^2$ (variance of error $\epsilon$). 
	\item For lease square models, Cp and AIC are proportional to each other P212. BIC statistic generally place a heavier penalty on models with many variables and hence results in the selection of smaller models than using Cp. The adjusted $R^2$ is not as well motivated in statistical theory as AIC, BIC and Cp. 
	\item Using CV to estimate test error has advantage relative to AIC, BIC, Cp, adjusted R2 in that it makes fewer assumptions about the true underlying model and can be used in a wider range model selection tasks, even in cases where it's hard to pinpoint the model degree freedom or hard to estimate the error variance $\sigma^2$.
	\item One standard error rule: we first calculate the standard error of the estimated test MSE for each model size and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve. (\textcolor{red}{SE of estimated test MSE for each model size???})
	find $\theta$ such that $CV(\theta) \le CV(\hat{\theta}) + SE(\hat{\theta})$, and $\hat{\theta}=\underset{\theta \in \theta_1,\ldots,\theta_m}{\mathrm{argmin} CV(\theta)}$.
	\item Ridge Regression: the ridge reg coef est $\hat{\beta}^R$ are the values that minimize
	\[ \sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2 +\lambda\sum_{j=1}^{p}\beta_j^2= RSS +\lambda\sum_{j=1}^{p}\beta_j^2
	\]
	use CV to get tuning parameter $\lambda$. $l_2$ norm of $\beta$ is defined as $||\beta||_2=\sqrt{\sum_{j=1}^{p}\beta_j^2}$, which measures the distance of $\beta$ from 0. As $\lambda$ increase, the $l_2$ norm of $\hat{\beta}^R$ will always decrease and so will $||\hat{\beta}^R||_2/||\hat{\beta}||_2$
	\item The LSEs are scale equivalent: multiplying $X_j$ by a constant c simply leads to a scaling of LSEs by a factor of $1/c$. In contrast, the ridge coef can change substantially when multiplying a given predictor by a constant. In other words, $X_j\beta_{j,\lambda}^R$ will depend not only on the value of $\lambda$, but also on the scaling of the j-th predictor, and may even depend on the scaling of other predictors. Therefore, it's best to apply ridge regression after standardizing the predictors, using 
	\[\bar{x}_{ij}=\frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_{ij}-\bar{x}_j)^2}}
	\]
	\item Ridge regression's advantage over least squares is rooted in the bias-variance trade-off. As $\lambda$ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias P217. Ridge reg works best in situations where the least squares estimates have high variance(when p is close to n). When $p > n$, the LSE do not have a unique solution, whereas ridge reg can still perform well by trading off a small increase in bias for a larger decrease in variance.
	\item Ridge regression also has substantial advantages over best subset selection, which requires searching through $2^p$ models. In contrast, for any fixed value of $\lambda$, ridge regression only fits a single model.One can show that the computations required for ridge regression to minimize the penalized equation, simultaneously for all values of $\lambda$, are almost identical to those for fitting a modeling using least squares.
	\item Lasso overcomes the problem that ridge regression will include all p predictors in the final model(ridge shrinks coef towards 0 but bot set to exactly 0 unless $\lambda=\infty$). The lasso coefficients, $\beta^L_{\lambda}$, minimize the quantity with $l_1$ penalty, which forces some of the coef est to be exactly equal to 0.
	\[	 \sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2 +\lambda\sum_{j=1}^{p}|\beta_j|= RSS +\lambda\sum_{j=1}^{p}|\beta_j|
	\]
    Therefore, lasso performs variable selection and yields sparse models.
    \item Another formulation for Ridge regression and Lasso
    \[	 \displaystyle{\min_{\beta}} (y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2 \textit{   subject to  } \sum_{j=1}^{p}\beta_j^2 \le s
    \]
     \[	 \displaystyle{\min_{\beta}} (y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2 \textit{   subject to  } \sum_{j=1}^{p}|\beta_j| \le s
    \]
    So, the ridge and lasso est have the smallest RSS out of all points that lie within the circle(p=2, ridge) or diamond(p=2, lasso) within s.
    \item The LSE is the result of following problem
    \[ \displaystyle{\min_{\beta}} (y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2 \textit{   subject to  } \sum_{j=1}^{p}I(\beta_j \ne 0) \le s
    \]
   It amounts to finding a set of coef est such that RSS is as small as possible, subject to the constraint that no more than s coef can be nonzero. Solving the above formulation requires considering all s from p models containing s predictors.Therefore, we can interpret ridge reg and lasso as computationally feasible alternatives to best subset selection that replace the intractable form of the budget in above formulation with forms that are much easier to solve.
   \item The lasso leads to feature selection due to the sharp corners of the polyhedron or ploytope P223, and results in models that are easier to interpret.
   \item Plotting MSE against $R^2$ on training data is a useful way to index models and can be used to compare models with different type of regularization.
   \item In general, Lasso will perform better in a setting where a relatively small number of predictors have substantial coefficients and the remaining have coef that are very small or that equal to 0. Ridge regression will perform better when the response is a function of many predictors, all with coef of roughly equal size. 
   \item Consider a special case with n=p and X is an identity matrix, and performing a linear regression without intercept. One can show that the ridge regression est take the form 
   \[ \hat{\beta}^R_j=y_j/(1+\lambda)
   \]
   and lasso est takes the form
	\begin{equation}
	\hat{\beta}^L_j =
	\begin{cases}
	y_j-\lambda/2 & \text{if} y_j > \lambda/2 \\
	y_j+\lambda/2 & \text{if} y_j < -\lambda/2 \\
	0 & \text{if} |y_j|\le \lambda/2
	\end{cases}
	\end{equation}
	In the case of a more general data matrix X, the main ideas still hold approximately: ridge regression more or less shrinks every dimension of the data by the same proportion, whereas the lass more or less shrinks all coefficients toward zero by a similar amount and sufficiently small coef are shrunken all the way to zero.
	\item Bayesian interpretation for ridge reg and the lasso: Assume that prior for $\beta$, $p(\beta)=\prod_{j=1}^{p}g(\beta_j)$, if g is Gassian distribution with mean 0 and std dev a function of $\lambda$, then it follows that the posterior mode for $\beta$ is given by the ridge reg solution.(and ridge solution is also the posterior mean). If g is a double-exponential(Laplace) distribution with mean 0 and scale parameter a function of $\lambda$, then it follows that the posterior mode for $\beta$ is the lasso solution(but the lasso solution is not the posterior mean).
	\item use CV to find the tuning parameter $\lambda$ for ridge and lasso
	\item Dimension reduction methods P229 transform the original predictors $X_1,\ldots, X_p$ to $Z_1,\ldots,Z_M$ with lower dimension $M < p$
	\[ Z_m = \sum_{j=1}^{p}\psi_{jm}X_j, m=1,\ldots,M.
	\]
	If the $\psi_{1m},\ldots,\psi_{pm}$ are chosen wisely, then regression on $Z_m$ with such dimension reduction can often outperform least squares regression.
	\[y_i=\theta_0+\sum_{m=1}^{M}\theta_mz_{im} + \epsilon_i, i = 1,\ldots,n.
	\]
	\[\sum_{m=1}^{M}\theta_mz_{im}=\sum_{j=1}^{p}\sum_{m=1}^{M}\theta_m\psi_{jm}x_{ij}=\sum_{j=1}^{p}\beta_jx_{ij}.
	\]
	\[\beta_j=\sum_{m=1}^{M}\theta_m\psi_{jm}.
	\]
	So dimension reduction serves to constrain the estimated $\beta_j$ coef, since now we must take the specific form  above for $\beta_j$ as above. This constraint has the potential to bias the coef est. But in situations where p is large relative to n, selecting a value of $M << p$ can significantly reduce the variance of the fitted coef.
	\item Principle component regression:
\end{itemize}
	
	
\section*{An Introduction to Statistical Learning Ch8 Tree}
\subsection{2/6/2018}
\begin{itemize}
	\item The regression tree is a over-simplification of the true relationship btw response and predictors but has the advantage over other types of regression like easy to interpret and has nice graphical presentation.
	\item prediction via stratification of the feature space: 
	\begin{itemize}
		\item[1] divide the predictor space into $J$ distinct and non-overlapping regions $R_1,R_2,\ldots,R_J$ that minimize the RSS, given by
		\[
		\sum_{j=1}^{J}\sum_{i\in R_j}(y_i-\hat{y_i})^2.
		\]
		\item[2] For every observation that falls into region $R_j$, we make the same prediction, which is simply the mean of the response values for the training observation in $R_j$.
	\end{itemize}
	\item Because it's computationally infeasible to consider every possible partition, we take a top-down, greedy approach known as  \textit{recursive binary splitting}. It's greedy because at each step of the tree-building process, the best split is make at that particular step, rather than leading to a global optimization. Splitting process continues until a stopping criterion is reached, for instance, until no region contains more than 5 observations.
	
	\item The regular decision tree is likely to overfit the data, leading to poor test set performance because the resulting tree might be too complex. We could set a higher threshold to get a small tree but this is too short-sighted since a seemingly worthless split may be followed by a very good split that leads to a large reduction in RSS later on.
	\item \textit{Cost complexity prunning(weakest link pruning)} is a better strategy that grows a large tree $T_0$ and then prune it back to obtain a subtree that leads to the lowest test error rate. Instead of considering every possible subtree, we consider a sequence of trees indexed by a nonnegative tuning parameter$\alpha$, for each $\alpha$ there corresponds a subtree $T\subset T_0$ such that 
	\[ \sum_{m=1}^{|T|}\sum_{i: x_i \in R_m }(y_i-\hat{y}_{R_m})^2 + \alpha|T|
	\]
	is as small as possible, $|T|$ indicates the number of leaves of tree T.(similar to the lasso) The process is summarized in Algorithm 8.1.
	\item Classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs to. Using classification error rate instead of RSS as the criterion for making a binary splitting. Classification error rate:
	\[ E=1-\displaystyle{\max_{k}}(\hat{p}_{mk})
	\]
	$\hat{p}_{mk}$ represents the proportion of training obs in the m-th region that are from the k-th class. However, E is not sufficiently sensitive for tree-growing. In practice, Gini index and cross-entropy.
	\item Gini index:\[
	G=\sum_{k=1}^{K}\hat{p}_{mk}(1-\hat{p}_{mk}) \]
	It's a measure of total variance across the K classes. Gini index takes on small values if all $\hat{p}_{mk}$ are close to 0, so Gini index is referred to as a measure of node purity: a small value indicates that a node contains predominantly obs from a single class.
	\item cross-entropy:\[
	D=-\sum_{k=1}^{K}\hat{p}_{mk} log(\hat{p}_{mk})\]
	It's always positive value. Similarly, it takes on value near to 0 when $\hat{p}_{mk}$ close to 0 or near 1 and the cross-entropy will take on small value if the m-th node is pure. Gini index and cross-entropy are quite similar numerically.
	\item Gini index and cross-entropy are typically used to evaluate the quality of a particular split, since the two are more sensitive to  node purity than is the classification error rate. All 3 can be used to prune a tree, the classification error rate is preferable if prediction accuracy of final pruned tree is the goal.
	\item why is node purity important? because we are more certain on the classification if the corresponding leaf is purer P314.
	\item Trees vs linear models, which one is better. It depends on the problem at hand. If the relationship btw the features and response, then Lm is likely to work better. If instead there is a highly non-linear and complex relationship, the decision tree may out-perform the classical approaches. In certain settings prediction using a tree may be preferred for the sake of interpretability and visualization.
	\item Trees generally don't have the same level of predictive accuracy as others; Trees can be very non-robust. A small change in the data can cause a larger change in the final estimated tree.
	\item Bootstrap is used in many situations in which it is hard or even impossible to directly compute the standard deviation of quantity of interest.
	\item The decision tree suffers from high variance: if we split the training data into two parts at random and fit decision tree to both halves, the results could be very different. Linear regression tends to have low variance if the ration of n to p is moderately large. 
	\item Bootstrap aggregation(bagging) is a general purpose procedure for reducing the variance of a statistical learning method, particular useful in the context of decision trees.
	\item Averaging a set of observations reduces variance. Bagging: First generate B bootstrap training data sets, by repeatedly sample from the single training data set, then train our method on b-th bootstrapped training set to get $\hat{f}^{*b}(x)$ and finally average all the predictions to get $\hat{f}_{bag(x)}$. For quantitative prediction, we simply average the predictions. For qualitative response, we can record the class predicted by each of the B trees, and take a majority vote: the overall prediction is the most commonly occurring class among the B predictions. The number of trees B is not a critical parameter with bagging: using a very large value B will not lead to over-fitting.
	\item  OOB error is the mean prediction error on each observation $x_i$, using only the trees that did not have $x_i$ in their bootstrap training sample. With B sufficiently large, OOB error is virtually equivalent to LOOCV error and OOB is particularly convenient when performing bagging on large set for which CV is computationally onerous P319.
	\item Bagging improves prediction accuracy at the expense of interpretability, since when we bag a large number of trees, it's no longer possible to represent the result using a single tree.
	\item We can obtain an overall summary of the importance of each predictor using the RSS(for bagging regression trees) or the Gini index(for bagging classification trees), by recording the total amount of reduction in RSS(or Gini) when splitting over given predictor, averaged over all B trees. A large value indicates an important predictor.
	\item Random forests provide an improvement over bagged trees by decorrelating the trees. Each time a split in a tree is considered, a random sample of m predictors is chosen as a split candidate from the full set of p predictors. The split is allowed to use only one of the m predictors and typically we choose $m \approx \sqrt{p}$.
	\item In random forests, it's not allowed to consider a majority of the available predictors. It makes sense b/c in bagging, if there is a very strong predictor, then in the collection of bagged trees, most or all the trees will use this strong predictor on top split. Consequently, all of the bagged trees will look quite similar(highly correlated). So random forest will lead to a substantial reduction in variance and more stable than bagging.
	\item Using a small m when building random forest will typically be helpful when we have many correlated predictors. As with bagging, random forests will not overfit if we increase B P321.
	\item Boosting, like bagging, it is a general approach that can be applied to many statistical learing methods for regression or classification. Boosting grows trees sequentially: each tree is  grown using information form previously grown trees. Unlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead learns slowly. Given the current model, we fit a decision tree to the residuals rather than y, and then add this new decision tree into the fitted function in order to update the residuals. Size of the trees d can be very small and shrinkage parameter $\lambda$ slows the process down even further, allowing more different shaped trees to attach the residuals. In general, statistical learning approaches that learn slowly tend to perform well. Boosting procedure is described in Algorithm 8.2. Boosting can overfit if B is large, so B is determined by CV. Shrinkage $\lambda$ has typical value 0.001 or 0.01. Very small $\lambda$ can require using a very large B to achieve good performance. The number of splits $d$ in each tree using 1 often works well P322 and leads to additive model(aid in interpretability)P323.
	\item Deviance in \textit{summary()} is given by 
	\[ -2\sum_{m}^{}\sum_{k}n_{mk}log\hat{p}_{mk}
	\]
	A small deviance indicates a tree that provides a good fit to the training data. The residual mean deviance is simply the deviance devided by $n-|T_0|$ P325. In regression tree, the deviance is simply the sum of squared error for the tree. We use cv.tree to see whether pruning the tree will improve performance P328.
	\item Bagging is simply a special case of random forest with $m=p$, \textit{randomForest(..., mtry=p)}, by default, the \textit{randomForest()}  uses $p/3$ variables when building a random forest of regression trees, and $\sqrt{p}$ variables when building a random forest of classification trees. \textit{importance()} to view the importance of variables. Two measures of importance are reported. \textit{IncMSE} is based upon the mean increased MSE in predictions on the out of bag samples when a given variable is excluded from the model.\textit{InccNodePurity} is a measure of the total increase in node purity that results from splits over that variable average over all trees P330.
	\item Use the \textit{gbm()} under \textit{gbm} package for boosting. P331, we can produce partial dependence plot for the most important two variables, which illustrate the marginal effect of the selected variables on the response after integrating out the other variables. The default shrinkage $\lambda=0.001$.
	
\end{itemize}
	
\section*{An Introduction to Statistical Learning Ch5 Resampling Methods}
\subsection{2/1/2018}
\begin{itemize}
	\item CV can be used to estimate the test error associated with a given statistical method in order to evaluate its performance(model assessment), or to select the appropriate level of flexibility(model selection).
	\item The validation set approach: It involves randomly dividing the available set of observations into two parts, a training set and a validation set or hold-out set. The validation set error rate typically assessed by MSE for quantitative response provides an estimate of the test error rate.
	\item Two drawbacks of the validation set approach:
	\begin{itemize}
		\item The validation estimate of the test error rate can be highly variable, depending on the splitting of training and validation sets.
		\item The validation set error rate may tend to overestimate the test error rate of the model fit on the entire data set(b/c only a subset of data used to train the model).
	\end{itemize}
	\item The LOOCV estimate for the test MSE is the average of n test error estimates:
	\[CV_{(n)}=\frac{1}{n}\sum_{i=1}^{n}MSE_i
	\]
	It has a couple of major advantages over the validation set approach
	\begin{itemize}
		\item LOOCV has far less bias since it use $n-1$ obs for training, which is almost the entire data set. So, LOOCV tends not to overestimate the test error rate as much as validation set approach.
		\item Performing LOOCV repeatedly will always yield the same results: there is not much randomness in the training/validation splits.
	\end{itemize}
	\item LOOCV has the potential to be expensive to implement, since the model has to be fit n times. With least squares linear or polynomial regression,
	\[ CV_{(n)}=\frac{1}{n}\sum_{i=1}^{n}\big(\frac{y_i-\hat{y_i}}{1-h_i}\big)^2,
	\]
	where $h_i$ is the leverage, which reflects the amount that an observation influences the fit. The formula is like the ordinary MSE except for the denominator. So the cost of LOOCV is the same as that of a single model fit P180.
	
	\item k-fold CV: In practice, one typically performs k-fold CV with k=5 or 10 to reduce computation load compare to LOOCV but gives more accurate estimates of the test error rate than does LOOCV P183; Variability in k-fold CV estimates(test error rates) is much smaller compare to validation set approach Figure 5.4 right vs. Figure 5.2 right. By comparing the CV estimate of test error rate and the true test error rate, using CV to identify minimum point in the estimated test MSE curve comes close to the actual correct level of flexibility Figure 5.6.
	\item From the perspective of bias reduction, LOOCV is to be preferred to k-fold CV because LOOCV use more observations for training; and the test error estimate from LOOCV tends to have higher variance than that from k-fold CV. Because, when we perform k-fold CV with $k<n$, we are average the outputs of k fitted models that are somewhat less correlated with each other, since the overlap between the training sets in each model is smaller than in LOOCV.
	\item The mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated. There is a bias-variance trade-off associate with the choice of k in k-fold CV, using k=5 or 10 has been shown empirically to yield test error estimates that suffer neither from excessively high bias nor from very high variance.
	\item Rather than using MSE to quantify test error, we instead use the number of misclassified observations.\[
	CV_{n}=\frac{1}{n}\sum_{i=1}^{n}Err_i,
	\] 
	where $Err_i=I(y_i\ne \hat{y_i})$, 
	
	\item Bootstrap: a powerful statistical tool can be used to quantify the uncertainty associated with a given estimate.
	\item For k-fold CV: \\ \verb|library(boot)|\\  \verb|glm.fit=glm(mpg~poly(horsepower,i),data=Auto)|\\
	\verb|cv.error.10[i]= cv.glm(Auto,glm.fit,K=10)delta[1]|
	
	\item For bootstrap:\\ \verb|alpha.fn=function{data,index}|\\  \verb|boot(Portfolio,alpha.fn,R=1000)|\\
	\verb|boot.fn=function{data,index} coefficients(lm(mpg~hp,data=data,subset=index))|
	
\end{itemize}

\section*{An Introduction to Statistical Learning Ch4 Classification}
\subsection{1/24/2018}
\begin{itemize}
	\item Why not use linear regression for classification? 1. Encoding the response as ordinal variable could produce fundamentally different linear models for different encoding, unless there is a natural ordering P130; 2. Even though binary response can be coded as dummy variable and use linear regression, $X\hat{\beta}$ is a estimate of $Pr(Y=1|X)$, it could be out of [0,1], make then hard to interpreted as probabilities. The classification using dummy variable for binary response will be the same as for the LDA procedure. 
	\item logistic function:
	\[p(X)=Pr(Y=1|X)=\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}
	\]
	logit function:
	\[log \big( \frac{p(X)}{1-p(X)} \big)=\beta_0+\beta_1X
	\]
	\item The intuition behind using maximum likelihood to fit a logistic regression model is as follows: we seek estimate of $\beta_0$ and $\beta_1$ such that the predicted probability$\hat{p}(y_i=1|x_i)$ corresponds as closely as possible to the individual's observed stuatus.The log-likelihood function:
	\[l(\beta_0,\beta_1)=\sum_{i:y_i=1}^{}p(x_i)\sum_{i':y_{i'}=0}^{}(1-p(x_{i'}))
	\]
	\item Confounding variables: the real effect of a predictor is masked(with opposite effect) by another variable, which is a result of collinearity among predictors in the example P136. From the web: A confounding variable is an outside influence that changes the effect of a dependent and independent variable; it can lead to erroneous conclusions about the relationship between the independent and dependent variables. 
	\item why use LDA instead of logistic regression when there are $> 2$ classes? LDA is more stable than the logistic regression when the classes are well separated, or when n is small and distribution of X is approximately normal. P138
	\item Use Bayes' Theorem for Classification: 
	\[ p_k(x)=Pr(Y=l|X=x)=\frac{\pi_kf_k(x)}{\sum_{l=1}^{K}\pi_lf_l(x)}
	\]
	just like in Bayes classifier, which classify an observation to the class for which $p_k(x)$ is largest, has the lowest possible error rate out af all classifiers. LDA is to approximate the Bayes classifier with estimated $f_k(x)$ and it assumes $f_k(x)$ is Gaussian and there is a common variance across all K classes. LDA classifier assumes that the observations in the k-th class are drawn from a multivariate Gaussian distribution $N(\mu_k,\Sigma)$. P143.
	\item Discriminant function for $p=1$ (1 predictor):
	\[\hat{\delta}_k(x)=x\cdot \frac{\hat{\mu}_k}{\hat{\sigma}^2}-\frac{\hat{\mu}_k^2}{2\hat{\sigma}^2} +log(\hat{\pi}_k)
	\]
	The word linear in LDA stems from the fact that the discriminant functions $\hat{\delta}_k(x)$ are linear functions of x. P141.
	\item Discriminant function for $p>1$ (matrix/vector version):
	\[\delta_k(x)=x^T\Sigma^{-1}\mu_k-\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k +log(\pi_k)
	\]
	\item Two caveats about the training error rate: P144.
		\begin{itemize}
			\item training error rates will usually be lower than test error rates, which are the real quantity of interest. The reason is that we specifically adjust the parameters of our model to do well on the training data. The higher the ratio of parameters p to number of samples n, the more we expect this overfitting to play a role.
			\item The data is very imbalanced(only 3.33\% in the training sample has y=1), The trivial null classifier will achieve a error rate that is only a bit higher than the LDA training set error rate.
		\end{itemize}
	\item LDA is trying to approximate the Bayes classifier, which has the lowest TOTAL error rate in the confusion matrix out of all classifier. That is, the Bayes classifier will yield the smallest possible total number of misclassified observations, irrespective of which class the errors come from P145. We can modify the threshold value for the posterior probability of Y=1 to save the business cost of misclassification, Figure 4.7 is a good visual tool, but the decide on the best threshold value must be based on domain knowledge about the associated cost. P147.
	\item ROC curve(Receiving Operating Characteristic) traces the two types of error as we vary the threshold value for the posterior probability of $Y=1$. The true positive rate on y-axis is the fraction of Y=1 classified correctly (which is called sensitivity). The false positive rate on x-axis is $1-specificity$, the fraction of Y=0 that are incorrectly classified as 1. The ideal ROC hugs the top left corner, indicating a high true positive rate and low false positive rate.
	\item The overall performance of a classifier, summarized over all possible thresholds is given by the AUC(area under the (ROC) curve), the larger the AUC the better the classifier. P147.
	\item Table 4.7 shows important measures for classification and diagnostic testing.
	\item QDA assumes that the observation from class $k$ are drawn from a Gaussian distribution $N(\mu_k,\Sigma_k)$, and assign an observation X=x to the class for which \[
	\delta_k(x)=-1/2x^T{\Sigma_k^{-1}}x + 1/2x^T{\Sigma_k^{-1}}\mu_k -1/2{\mu_k}^T{\Sigma_k^{-1}}\mu_k -1/2 log|\Sigma_k| + log\pi_k
	\]
	is largest.
	\item why would one prefer LDA  to QDA? LDA is a much less flexible classifier than QDA and so has substantial lower variance. But if LDA's assumption that K classes share a common covariance matrix is badly off, then LDA can suffer from high bias. Roughly speaking, LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance if crucial. In contrast, QDA is recommended if the training set is very large or if the assumption of common covariance matrix for the K classes is clearly untenable (unjustifiable).
	\item A comparison of 4 classifiers: Logistic regression, LDA, QDA and KNN. Both Logistic and LDA produce linear boundaries, the only difference btw them lies in the fact that parameters in logistics reg are estimated by MLE, whereas parameters in LDA computed using the estimated mean and covariance of normal distribution. KNN is a completely non-parametric approach: no assumptions are made on the decision boundaries. KNN would dominate logistic reg and LDA in prediction when the decision boundary is highly non-linear. But KNN would not tell which predictor are important. QDA serves as a compromise btw non-parametric KNN and the linear LDA/logistic. Since QDA assumes a quadratic boundary, it can model a wider range of problems than linear methods, though not as flexible as KNN.
	\item In the logistic regression setting we can accommodate a non-linear relationship btw the predictors and response by performing regression using transformation of predictors(e.g., by including $X^2$). This may or may not improve the performance, depending on whether the increase in variance due to the added flexibility is offset by a sufficiently large reduction in bias. We could do the same for LDA, If added all possible quadratic terms and cross-product to LDA, the form of model would be the same as QDA, though the parameter estimates would be different. This allow us to move somewhere between LDA and QDA.
	\item Using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors can cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement. P160.
	\item For KNN, the scale(unit) of the variables matters on the distance! Any variables that are on a large scale will have a much larger effect on the distance btw the observations, and hence on the KNN classifier. P165. A good way to handle this problem is to standardize the data so that all variables are given a mean of zero and a standard deviation of one, so that on a comparable scale.
	
\end{itemize}


\section*{An Introduction to Statistical Learning Ch3}
\subsection{1/15/2018}
\begin{itemize}
	\item An unbiased estimator does not systematically over- or under-estimate the true parameter. P65
	\item In $SE(\hat{\beta_0})$ and $SE(\hat{\beta_1})$, the error variance $\sigma^2$ is estimated by $RSE=\sqrt{(RSS)/(n-2)}$. P66
	\item a small p-value indicates that it's unlikely to observe such a substantial association btw the predictor and the response due to the chance, in the absence of any real association btw the predictor and the response. So if we see a small p-value, we can infer there is an association btw the predictor and the response. P67
	\item To quantify the extent to which the model fits the data: Residual Standard Error(RSE) and $R^2$. The RSE is an estimate of the standard deviation of $\epsilon$. It is roughly the average amount that the response will deviate from the true regression line in the units of y. It measures the lack of fit of model to data. The paragraph under (3.16) on P69 illustrate the meaning of it with example. 
	\item $R^2$ measures the proportion of variety in y can be explained using x. And it is a measure of the linear relationship btw x and y. In SLR, $R^2=r^2$, where r is the correlation btw x and y.	
	\item The optimal K in KNN will depend on the bias-variance tradeoff: smaller K has large bias, and larger K has higher variance P105. Bias refers to the error that is introduced by approximating a real-life problem, which maybe extremely complicated, by a much simpler model Ch2 P35. Variance refers to the amount by which $\hat{f}$ will change if we estimated it use a different training data set Ch 2 P34. As a general rule, as we use more flexible methods(more predictors, or larger $1/K$), the variance will increase and the bias will decrease P35.
	
	\item  The parametric approach will outperform the nonparametric approach if the parametric form that has been selected is close to the true form of $f$. P106
	\item Curse of dimensionatily to make KNN a poor prediction: the K observations that are nearest to a given test observation $x_0$ may be very far away from $x_0$ when $p$ is large comparing to the distance when $p=1,2$ P108.
	
\end{itemize}
	
\section*{An Introduction to Statistical Learning Ch2}
\subsection{12/17/2017}
\begin{itemize}
	\item How to estimate f: Parametric and Non-parametric
	\item The order of trade-off btw flexibility(prediction accuracy) and interpretability: Sebset selection, Lasso; Least square; Generalized additive models, Trees; Bagging, Boosting; Support vector machines.
	\item Generalized additive models(GAMs) are more flexible than linear regression because the relationship btw each predictor and the response is now modeled using a curve. \textcolor{red} {why it's a curve? allow higher order?}
	\item For unsupervised learning, we can seek to understand the relationships btw variables or between the observations. One tool is cluster analysis or clustering. ascertain： 弄清，确定，查明
	\item MSE(Mean squared error) is the most commonly-used measure for quality of fit in regression setting.
	\item Expected test MSE can be decomposed as Bias+ Variance + variance of irreducible error in (2.7)
	\[
	E(y_0-\hat{f}(x_0))^2=Var(\hat{f}(x_0)) +[Bias(\hat{f}(x_0))]^2 +Var(\epsilon)
	\]
	\item The Bayes Classifier assign each observation to the most likely class given its predictor value$Pr(Y=j|X=x_0)$. It has the minimum test error rate in (2.9) $	Ave(I(y_0\ne \hat{y_0}))$(the error rate associate with the test set) on average P37. The Bayes classifier produces the lowest possible test error rate, called the Bayes error rate: 
	\[
	1-E(\displaystyle{\max_{j}} Pr(Y=j|X)),
	\]
	where the expectation averages the probability over all possible values of X. \textcolor{red}{(how to do this average?)}
\end{itemize}


	
	
	

\section*{Chapter 1} %the star prohibited the automatic numbering
\subsection*{6/16/2017}

1. Supervised learning: having the presence of outcome variable to guide the learning process. Unsupervised learning:have no measurement of the outcome, our task is clustering.

2. Classification problem(predict categorical output)and regression problem(predict quantitative output)are Supervised learning;while clustering is Unsupervised learning.

\section*{Chapter 2}
\subsection*{6/18/2017}

1. Linear model  makes huge assumption, yield stable but probably inaccurate predictions(low variance and high bias); while K-nearest neighbors model make mild assumptions, its predictions often accurate but unstable

2. \textcolor{red}{ P12,  1st paragraph, what is affine set? 	2nd paragraph, why steepest uphill direction?}

\textbf{Affine set} A subspace $S$ is a line or a plane go through the origin, i.e., $0\in S$. A subspace can be represented by a linear combination of basis vectors $b_k$'s
$$S = \{\lambda_1 b_1+\cdots+\lambda_p b_p, \lambda_k\in R\}$$.
If we move this subspace by an offset $o\notin S$ (or in the direction of $o$), we will get an affine set
$$A = o+S = \{o+\lambda_1 b_1+\cdots+\lambda_p b_p, \lambda_k\in R\}$$.

For what is discussed in the book, consider the simplest example with only one predictor
$$
y = \beta_0 + \beta_1 x
$$
\begin{itemize}
	\item If the constant is included, then each point is $z = (1,x,y)^T$. Let $\gamma = (\beta_0, \beta_1, -1)$. Then the point $z$ is in the subspace defined by $S = \{z: z^T\gamma = 0\}$.
	\item  If the constant is not included, then each point is $z = (x,y)$. Let $\gamma = (\beta_1, -1)$. Each point is in the affine set defined by $A = (0, \beta_0)^T + \{
	z: z^T\gamma = 0\}$. The offset is $o=(0, \beta_0)^T$.
\end{itemize}

\vspace{10pt}
\noindent \textbf{Greatest uphill direction}. By first order Tylor expansion,
$$
f(X+\Delta) - f(X) \approx \langle f'(X) , \Delta \rangle = \Vert f'(x)\Vert \Vert \Delta\Vert \cos\theta
$$
where $\langle \va,\vb\rangle = \sum_i a_i b_i$ is the inner product of vector 
$\va$ and $\vb$. Inner product is equal to the vector norm mutiplied by $\cos\theta$, and $\theta$ is the angle between the two vectors. If $\Delta$ is in the same direction as $f'(x)=\beta$, then $\theta = 0$ and $\cos\theta = 1$. The increase of function value $f(X+\Delta) - f(x) $ is greatest.  

Image we are climbing a mountain, our current position is $X$ and the height is $f(X)$. We wish to go to the top of the mountain. 
If we walk from $X$ in the direction of $\beta$, we get greatest increase in $f(X)$ (uphill direction).

3. The error on the training dataset should be approximately an increasing function of k for k-nearest neighbor fit; cannot use sum of squared error as a criteria for picking k.

4. The effective number of parameters in k-nearest neighbor is N/k, i.e., if non-overlapping, we need fit N/k means for totally N/k neighborhoods

5. If the training data is simulated from independent Gaussian distributions, least square is appropriate for prediction; if from mixture Gaussian, k-nearest neighbor is better.
\subsection*{6/19/2017}

1. Bullets on P17,P18: kernel methods, local regression ... enhanced the simple 1(k)-nearest neighbor procedure. Kernel methods having weights decrease smoothly to zero with distance from the target point, rather than the 0/1 weights used by k-nearest neighbors.

2. Minimize the squared prediction error(EPE) loss, $EPE(f)= E_xE_{Y|X}([Y-f(X)]^2|X)$, get solution $f(x)= E(Y|X=x)$, i.e., the conditional expectation(known as the regression function). Nearest-neighbor approximates $\hat{f}(x)=Ave(y_i|x_i \in N_k(x))$. Under mild condition on P(X,Y), as N,k $\rightarrow \infty$ such that k/N $\rightarrow$ 0, $\hat{f}(x) \rightarrow E(Y|X=x)$. The convergence still holds but the rate of convergence decreases as the dimension p increases.

3. Additive model solves the high dimensional problem by approximating the universal conditional expectation simutaneously for each of the coordinate function $f_j$ in $f(X)=\sum_{j=1}^pf_j(X)$.

4. If $L_1$ loss: $E|Y-f(X)|$, the solution will be conditional median $f(x)= median(Y|X=x)$. The discontinuity in its derivatives hendered their wide use.

5. Loss function for categorical prediction is represented by a matrix L=card(G) with 0 on the diagonal and nonnegative elsewhere(often 1s).
\textcolor{red}{ How to calculate (2.19) - (2.23)?}
\vspace{6pt}
\noindent \textbf{Equation 2.19--2.23}. 
(2.19) takes expectation with respect to both $X,\ G$. 
$$
\mathrm{EPE} = E_{X,G}[L(G,\hat{G}(X)]
$$
(2.20) uses the following relation
$$
\mathrm{EPE} = E_X\Big[ E_G[L(G,\hat{G}(X) | X]\Big]
$$
The conditional expection inside is replace by the summation in
(2.20). Then (2.21) tries to minimize the above risk to get an
estimator $\hat{G}$. (2.22) and (2.23) are the solution for 0-1 loss
function. It classifies $X$ to the most probable class. 

\subsection*{6/20/2017}

1. Curse of dimensionality: (1) To capture a fraction r of a p-dimensional hypercube,  the expected edge length will be $r^{1/p}$, i.e., to capture $1\%$ of the data forming a local hypercube, we need to cover $63\%$ of the range in each dimension/variable of the input; 

(2)Mean distance of the closest point increases for a large $p$ so the prediction is more difficult near the edges of the training sample;

(3)Sampling density is proportional to $N^{1/p}$, i.e., $100^10$ sample needed for p=10 to get same sample density of 100 in 1D.

(4)The complexity of function(Y=f(X)) with many variables(larger p) can grow with the dimension p, and if we want to estimate such functions with same accuracy as in low dim, then we need the size of training data grow exponentially.

2.Calculation in (2.27) shows why prediction error have additional variance $\sigma^2$.

3. If we assume it's a linear model, (2.28) shows the expected EPE increase linearly in p. By imposing linear restrictions, we have avoided the curse of dimensionality.

\subsection*{6/21/2017}

1. \textcolor{red}{ how to get the $\hat{y}_0$ on the bottom of page 24?}

\vspace{6pt}
\noindent \textbf{Equation 2.26, ($\hat{y}_0$)}. 
\begin{eqnarray*}
	\hat{y}_0 &=& x_0^T\hat{\beta} = x_0^T (X^TX)^{-1} X^Ty\\
	& =& x_0^T (X^TX)^{-1} X^T (X\beta + \epsilon) \\
	& = & x_0^T (X^TX)^{-1} X^T X\beta +x_0 (X^TX)^{-1} X^T X \epsilon \\
	& = & x_0^T \beta +x_0 (X^TX)^{-1} X^T X \epsilon 
\end{eqnarray*}

2. figure 2.9 shows comparison of least square and 1-nearest neighbor with their ration for two senario(get the way of comparison). There is acutally a whole spectrum of methods between the rigid linear models and the extreme flexible 1-nearest neighbor models. Section 2.6 using other f(X), specifically designed to overcome the dimensionality problem.

3. In additive error model,$Y=f(X)+\epsilon$,$\epsilon$ is independent of X.More often, (X,Y) will not have a deterministic relationship $Y=f(X)$, so the additive model assume that we can capture all the departures from a deterministic relationship via the error $\epsilon$.

4. \textcolor{red}{ 
In the beginning of 2.6.1, Why the conditional distribution  $P(Y|X)$ depends on $X$ only through the conditional mean $f(x)=E(Y|X=x)$? Answered in equation (2.34).
}

\vspace{6pt}
\noindent \textbf{Section 2.6.1}
$Y = f(X) + \epsilon$. Only $\epsilon$ is random and its distribution
is known or fixed. The $f(X)$ shifts the mean of $\epsilon$. 
Given $X$, we know $f(X)$, so we also know the distribution of $Y$. 

5. Two ways of understanding: Supervised learning VS function approximation. Learning by example: the learning algorithm modify its input/output relationship in response to the prediction error; In terms of function approximation, we imagine our parametrized function as a surface in p+1 dim space and we observed noisy representations from it.

6. The principle of MLE assumes that the most reasonable values for $\theta$ are those for which the probability of the observed sample is largest.



\subsection*{6/22/2017(Ch2 P32-P34)}

1. need Approaches to make efficient use of structured data. There are infinitely many solutions for $\hat{f}$, so we impose restritions on eligible solutions.

2. Most training methods imposed complexity restrictions in small neighborhood of input space, e.g., $\hat{f}$ exhibits special structure such as nearly constant, linear or low order polynomial behavior in the small neighborhood.

3. Any method that attempt to produce locally varying functions in small isotropic neighborhood will run into problems in high dimension; all methods that overcome the dimensionality problems have an associated metric for measuring neighborhoods, which does not allow the neighborhood to be simutaneously small in all directions.

4. nonparametric methods falls in several classes and each of the classes has associated with one or more smoothing parameters that control the effective size of the local neighborhood.

5. \textcolor{red}{ 
why called adaptively chosen basis function methods in the last and second last paragraph of page 36?
}

\subsection*{6/23/2017(Ch2 P34-P38)}

1. Roughness penalty controls the second derivative of $f$, to get a smooth solution. Penalized least-sqaure criterion $$PRSS(f;\lambda)= \sum_{i=1}^N(y_i -f(x_i))^2 + \lambda\int[f''(x)]^2dx$$. when $\lambda=\infty$, only linear function(the most smooth) are permitted. 

2. Penalty function, or regularization methods express our prior belief that the type of functions we seek exhibit smooth behavior.

3. Kernal methods specified the nature of the local neighborhood. Kernal function $K_{\lambda}(x_0,x)$ assigns weights to points x in a region around $x_0$. Define a local regression estimate $f_{\hat{\theta}}(x_0)$ where $\hat{\theta}$ minimizes $$RSS(f_{\theta},x_0)=\sum_{i=1}^NK_{\lambda}(x_0,x_i)(y-f_{\theta}(x_i))^2$$

4. Basis functions: the model $f$ is a linear expansion of basis functions $$f_{\theta}(x)=\sum_{m=1}^M\theta_mh_m(x)$$
The term linear refers to the action of parameter $\theta$.

5.  \textcolor{red}{ 
Didn't understand the 2nd paragraph on Page36, reread it after reading Ch5. Spline basis has knots. In general, we would like the data to dictate them as well but that will lead to a hard nonlinear problem.
}

6. All the above models have a smoothing or complexity parameter: The multiplier of the penalty term; the width of the kernel; the number of basis functions. We cannot use residual sum-of-squares on the training data to determine these parameters since it will always pick the interpolation fits hence zero residuals.

7. As the model complexity increases, the variance tends to be increase and bias decreases. k-nearest neighbor as k increase, variance decrease and squared bias increases.

8. Figure 2.11 shows the typical behavior of test error and training error: as model complexity increase, the training error would decrease since the model adapts itself too closely to the training data(overfitting) and will not generalize well(large test error).

\section*{Chapter 3}

\subsection*{6/24/2017(Ch3 P43-P46,Notes made up on 6/25)}

1. Linear model assumes that the regression function $E(Y|X)$ is linear \textcolor{red}{  in X not in $\beta$! } Linear models can be applied to transformations(like the basis) of the input, which considerably expand their scope.

2. Even if the $x_i$ were not drawn randomly, the least square criterion is still valid if the $y_i$'s are conditionally independent given the input $x_i$.\textcolor{red}{ (Did not assume normality in LSE, right?) }

3. The estimate $\hat{y}$ is the orthogonal projection of y onto the subspace spanned by x, and residual vector $y-\hat{y}$ is orthogonal to this subspace.

4. If X is not full rank,$X^TX$ is singular and the least square coefficients $\hat{\beta}$ are not uniquely defined. However, the fitted values $\hat{y}$ are still the projection of y onto the column space of X; there is just more than one way to express that projection in terms of X.

5. Rank deficiency can also occur in signal and image analysis, where $p>N$. In this case, the features are typically reduced by filtering or controlled by regularization.

\subsection*{6/25/2016 (Ch3 P46-51)}

1.$$Var(\hat{\beta})=(X^TX)^{-1}\sigma^2$$
To  draw inference about the parameters and the  model, need additional Gaussian assumption
$$\hat{\beta} \sim N(\beta,(X^TX)^{-1}\sigma^2 )$$
$$(N-p-1)\hat(\sigma^2) \sim \sigma^2\chi^2_{N-p-1}$$
$\hat{\beta}$ and $\hat{\sigma}^2$ are statistically independent.

2. Equation 3.13, the F statistics measures the changes in residual sum-of-squares per additional parameter in the bigger model, and it is normalized by an estimate of $\sigma^2$. The F statistics for dropping a single coefficient from a model is equal to the square of the corresponding z-score(3.12), since $t_{\nu}^2=F_{1,\nu}$

3. Even if the Gaussian error assumption not hold, the CI will be approximately correct with its coverage approaching $1-2\alpha$ as sample size $N \rightarrow \infty$.

4. calculate z-score for all $\hat{\beta}$'s, a z-score greater than 2 in absolute value is approximately significant at the $5\%$ level.

\subsection*{6/26/2016 (Ch3 P51-P55)}

1. The LSE of parameter $\beta$ have the smallest variance among all linear unbiased estimates. Restricted to unbiased estimates is not necessary wise. From a pragmatic point of view, most models are distortion of the truth and hence are biased; picking models amounts to creating the right balance btw bias and variance.

2. The multiple regression coefficient $\hat{\beta}_j$ represents the additional contribution of $x_j$ on y, after $x_j$ has been adjusted for $x_0,x_1,\ldots,x_{j-1},x_{j+1},\ldots,x_p$.

3. For univariate model, $\hat{\beta}=\frac{<x,y>}{<x,x>}$. Suppose that columns of X are orthogonal, them the multiple least square estimates $\beta_j=\frac{<x_j,y>}{<x_j,x_j>}$ the univariate estimates, i.e., when inputs are orthogonal, they have no effect on each other's parameter estimates. Algorithm 3.1 Page 54 is known as Gram-Schmidt procedure for multiple regression(orthogonalized the inputs X):
\begin{itemize}
	\item $z_0 =\textbf{1}$, regress $x_1$ on $z_0$, $x_1=z_0\gamma_{01} + e$, get residual $z_1=x_1-z_0\hat{\gamma}_{01}$, regress y on $z_1$ get $\hat{\beta}_1$
	\item regress $x_2$ on $z_1,z_0$, get residual $z_2=x_2-z_1\hat{\gamma}_{12}-z_0\hat{\gamma}_{02}$, regress y on $z_2$ get $\hat{\beta}_2$
	\item $\ldots$
\end{itemize}


\subsection*{6/27/2016 (Ch3 P55-P57)}

1. We can represent step 2 of Algorithm 3.1 in matrix form: $X=Z\Gamma$, where z has columns $z_j$ in order and $\Gamma$ is the upper triangular matrix with entries$\hat{\gamma}_{k,j}$. Diagonal matrix D with j-th diagonal entry $D_{jj}=||z_j||^2$ to standardize Z in $X=ZD^{-1}D\Gamma=QR$.$Q^TQ=I$. QR decomposition represents a convenient orthogonal basis for the column space of X. Easy to get(with R being upper triangular matrix).
$$\hat{\beta}= R^{-1}Q^Ty; \hat{y}=QQ^Ty$$

2. For multivariate output Y, $RSS(B)=tr((Y-XB)^T(Y-XB))$, $\hat{B}$ same format of LSE as for univariate y \textcolor{red}{even if $cov(\epsilon)=\Sigma$, why? (not the case if $\Sigma_i$ differ for observations i)}.

\subsection*{6/28/2016 (Ch3 P57-P62)}

1. Backward selection can only be used when $N>p$, while forward selection can always be used. Hybrid stepwise selection consider both forward and backward moves at each step, and select the "best" of the two.

2.  forward-stagewise regression starts with an intercept equals to $\overline{y}$ and centered predictors with coefficients initially at 0. At each step, it identifies the variable most correlated with the residual and then compute the SLR coefficient on the chosen variable and adds it to the current coefficient for that variable. It will continue until none of variables have correlation with residuals. Slow but competitive for high-dim problems.

3. description of 10 fold cross validation 3.3.4 Page61 1st paragraph. 

4. Model selection is a discrete process and often exhibit high variance. Shrinkage methods are more continuous and don't suffer as much from high variability.

\subsection*{6/29/2016 (Ch3 P63-P67)}

1. The main motivation for ridge regression when first introduced is to make $X^TX$ nonsingular by adding a $\lambda$. In the case of orthogonal inputs X, the ridge regression estimate $\beta_{ridge}=\hat{\beta}/(1+\lambda)$. The ridge estimate is the mode or mean of the posterior distribution of $\beta$, with $\lambda=\sigma^2/\tau^2$, prior $\beta \sim N(0,\tau^2)$.

2. \textcolor{red}{What conclusion can be draw from the profile plot of ridge regression Page64 Figure 3.8}

3. Page 66 connect ridge regression,SVD, LSE and principle component, very helpful!!! Ridge regression shrinks the coordinates by factor of $d^2_j/(d^2_j+\lambda)$, which means a greater shrinkage is applied to the coordinate of basis vectors with smaller $d^2_j$, which corresponding to smallest principle component direction, i.e., the direction in the column space of X having smallest variance. The small singular value corresponding to the direction in the column space of X having smallest variance.

\subsection*{6/30/2016 (Ch3 P67-P70)}

1. The implicit assumption for PCA or ridge regression is that the response will tend to vary most in the directions of high variance of the input, which is often reasonable since predictors are often chosen for study because they vary with the response variable.

2. effective df of ridge reg: $df(\lambda)=\sum_{j=1}^{p}\frac{d^2_j}{d^2_j+\lambda}$. $df(\lambda)=p$ when $\lambda=0$,$df(\lambda)=0$ when $\lambda=\infty$

3. $L_2$ ridge penalty $\sum_{1}^{p}\beta^2_j$ is replaced by $L_1$ penalty $\sum_{1}^{p}|\beta_j|$, which makes the lasso solution nonlinear in $y$ and having no explicit expression.

4. The standardized tuning parameter $s=t/\sum_{1}^{p}|\hat{\beta}^{LS}_j|$ indicate the percentage that LSE coefficients are shrunk on average. At $s=1$, lasso gives LSE; and the estimates decrease to 0 as $s \rightarrow 0$.

5. profile plot for Lasso is piece-wise linear(less computation) and it hits 0 while ridge does not.

\subsection*{7/1/2017 (Ch3 P69-P73)}

1. Lasso is "soft-thresholding"; Best-subset selection is a form of "hard-thresholding".

2. Figures on Page 71 is useful!

3. We can generalize ridge regression and the lasso and view then as Bayes estimates.
$$\tilde{\beta}=argmin{\sum_{i=1}^{N}(y_i-\beta_0-\sum_{j=1}^{p}x_{ij}\beta_j)^2 +\lambda\sum_{j=1}^{p}|\beta_j|^q}$$
q=1 corresponding to lasso with double exponential or Laplace (log)prior for $\beta_j$ with density$(1/2\tau exp(-|\beta|/\tau))$ and $\tau=1/\lambda$; q=2 to ridge regression. Ridge regression is the posterior mean but lasso and best-subset are not.

4. q=1(lasso) is the smallest q such that the constraint region is convex.

5. with $q>1$,$|\beta_j|^q$ is differentiable at 0(no corner on the constraint region), so does not share the ability of lasso for setting coefficients exactly to 0. Partially for this reason as well as for computational tractability, elastic-net is proposed, a compromise between lasso and ridge.

\subsection*{7/2/2017 (Ch3 P73-P79)}

1. LAR "least angle regression" provides an extremely efficient algorithm for computing the entire lasso path as in Figure 3.10.

2. By construction, the coefficients in LAR change in a piecewise linear fashion. Using knowledge of the covariance of the predictors and the piecewise linearity of the algorithm 3.2(LARS) on Page74, we can figure out the exact step length(like the joint point in Figure 3.14). "Least Angel Regression and Shrinkage"

3. Algorithm 3.2a(LAR) gives the entire lasso path.

4. In classical statistics, the number of linearly independent parameters is what is meant by "degree of freedom". A more general definition of the effective degrees of freedom of an adaptively fitted model:$$df(\hat{y})=\frac{1}{\sigma^2}\sum_{i=1}^{N}cov(\hat{y}_i,y_i)$$
Intuitively, the harder that we fit the data, the larger this covariance and hence $df(\hat{y})$.

\subsection*{7/3/2017 (Ch3 P79-P83)}

1. Principle components regression(PCR) forms orthogonal inputs $z_m=Xv_m$ for $m=1,\ldots,M \le p$. if M=p, we will get back the usual LSE since the columns of $Z=UD$ span the column space of X. If $M < p$ we get reduce regression like ridge: ridge regression shrinks the coef of the PCs, shrinking more depend on the size of the corresponding eigenvalues; principle components regression discards the $p-M$ smallest eigenvalue components.

2. Partial least squares(PLS) also construct a set of linear combination of the inputs but also uses $y$(unlike PCR), produces a sequence of derived, orthogonal inputs as in Algorithm 3.3. The PLS seeks directions that have high variance AND high correlation with the response, while PCR only on high variance. 

3. \textcolor{red}{ what is the rationals behind construction as $z_m=\sum_{j=1}^{p}\psi_{mj}x_j^{(m-1)} $ in PLS, for orthogonality?}

4. PLS, PCR and ridge regression behave similarly, ridge regression is preferred because it shrinks smoothly. Lasso fall between ridge and best subset, and enjoys some properties of each.

\subsection*{7/4/2017 (Ch3 P86-P93)}

1. Incremental Forward Stage-wise regression generates a coefficient profile by repeatedly updating(by a small amount $\epsilon$) the coefficient of the variable most correlated with the current residuals. Letting $\epsilon \rightarrow 0$ could give identical lasso path. \textcolor{red}{$\delta_j=<x_j,r>$(the least squares coefficients of the residual on jth predictor!!)}

2. The grouped lasso is to shrink and select the members of a group together by minimizing the convex criterion:$$\displaystyle{\min_{\beta \in R^p}}\Bigg(||y-\beta_01-\sum_{l=1}^{L}X_l\beta_l||_2^2 + \lambda\sum_{l=1}^{L}\sqrt{p_l}||\beta_l||_2\Bigg)$$

3. Caomputation of lasso via the LAR algorithm has the same order of computation as a least square fit.

\section*{Chapter 4}

\subsection*{7/5/2017 (Ch4 P101-P105)}

1. The regression approach is a member of a class of methods that model discriminant functions $\delta_k(x)$ for each class, and then classify x to the class with the largest value for its discriminant function. To get linear boundaries, all we need is that some monotone transformation of $\delta_k$ be linear, like logit transformation for logistic regression, where the decision boundary is the set of points for which the log-odds are zero.

2. Figure 4.1 shows the scenario that linear functions in the augmented space map to the quadratic function in the original space, when using with basis functions.

3. \textcolor{red}{why $\sum_{k \in \mathcal{G}}^{}\hat{f}_k(x)=1$ as long as there is intercept?Page104 2nd paragraph. $\hat{f}_k$ can be negative or larger than 1 as a consequence of the rigid linear regression}

4. \textcolor{red}{Page 104 Did not really understand after paragraph 2... till end of page 106 }

\subsection*{7/6/2017 (Ch4 P104-P109)}

1. main information extracted from 104-106: linear regression model for indicator could mask some categories like in Figure 4.2, while LDA works better.  \textcolor{red}{Did not understand Figure 4.3}, but the main idea is that for $K \ge 3$, using polynomials of order $K-1$ is good.

2. LDA arises in the special case when we assume that the classes have a common covariance matrix. Checking the log-ratio of two classes is linear in x so that the class boundary is linear.

\subsection*{7/7/2017 (Machine Learning - A probability perspective 4.2-4.2.2}

1. Gaussian discriminant analysis: define the class conditional density using multivariate Gaussian. $$p(x|y=c,\theta)=N(x|\mu_c,\Sigma_c)$$. If $\Sigma_c$ is diagonal, this is equivalent to Naive Bayes.

2. Naive Bayes classifier: Assume the features are conditionally independent given the class label. $p(x|y=c,\theta)=\prod_{j=1}^{D}p(x_j|y=c,\theta_{jc})$, D is the number of features. work well even if independent assumption not valid. Simple model with only $O(C*D)$ parameters hence relatively immune to overfitting.

3. generative classifier(Equation 2.13) specify how to generate the data using the class prior $p(y=c)$, class conditional density $p(x|y=c)$.
$$p(y=c|x)=\frac{p(y=c)p(x|y=c)}{\sum_{c'}^{}p(y=c')p(x|y=c')}$$

4. $\hat{y}(x)=\underset{c}{\mathrm{argmax}} p(y=c|x)= \underset{c}{\mathrm{argmax}} logp(y=c) + logp(x|y=c) $ The later part measures the distance from x to the center of each class, $\mu_c$, using Mahalanobis distance. For a uniform prior, $\hat{y}=\underset{c}{\mathrm{argmin}} (x-\mu_c)^T\Sigma_c^{-1}(x-\mu_c))$.

5. quadratic discriminant analysis is a typical Gaussian discriminant analysis.

6. linear discriminant analysis is for $\Sigma_c=\Sigma$, the covariance matrices are tied or shared across classes. Quadratic part canceled out between numerator and denominator. Taking log will give a linear function of x, the decision boundary is linear. $p(y=c|x)$ can be written as a softmax function: $\frac{e^{\eta_c}}{\sum_{c'=1}^{C}e^{\eta_{c'}}}$. Devide $\eta_c$ by Temperature T, at low T, the distribution stay in the most probable state, while at high T, it visits all states uniformly(Figure 4.4), like Boltzmann distribution.
	
\subsection*{7/9/2017 (Machine Learning - A probability perspective 4.2.3}

1. \textcolor{red}{Did not understand the Figure 4.6 on Page 105, 2nd paragraph on Page 106.}

2. (Element of Statistical Learning Ch4 P109) In practice we don't know the parameters of the Gaussian distributions, and will need to estimate them using our training data.

3. The linear discriminant functions (quadratic part $x^T\Sigma^{-1}x$ was canceled)
$$\delta_k(x)=x^T\Sigma^{-1}\mu_k-\frac{1}{2}\mu^T_k\Sigma^{-1}\mu_k + log\pi_k$$
	
\subsection*{7/10/2017 Ch4 P109- P113 }

1. There is simple correspondence between LDA and classification by LS: the coefficient vector from least squares is proportional to the LDA direction, unless N1=N2, the intercepts are different and hence the resulting decision rules are different.

2. With more than 2 classes, LDA is different with linear regression of class indicator matrix and it avoids masking problem as in Figure 4.2 P105.

3. quadratic discriminant functions(QDA): 
$$\delta_k(x)=-\frac{1}{2}|\Sigma_k| - \frac{1}{2}(x-\mu_k)^T\Sigma^{-1}_k(x-\mu_k) +log\pi_k$$
The decision boundary is described by a quadratic equation${x:\delta_k(x)=\delta_l(x)}$

4. For LDA, there seems $(K-1)\times(p-1)$ parameters. In QDA, there will be $(K-1)\times{p(p+3)/2+1}$ parameters.

5. we should always have LDA and QDA available as good track record, because the data can only support simple decision boundaries such as linear or quadratic and the estimates provided via the Gaussian models are stable.

6. Regularized Discriminant Analysis allows one to shrink the separate covariance of QDA towards a common covariances as in LDA. The methods are similar to ridge regression. $\hat{\Sigma}_k(\alpha)=\alpha\hat{\Sigma}_k + (1-\alpha)\hat{\Sigma}$ where $\hat{\Sigma}$ is the covariance matrix used in LDA and $\alpha$ can be chosen by cross-validation.

\subsection*{7/12/2017 Ch4 P113- P113 }

1. In light of = based on

2. Computation of QDA is simplified by diagonalizing of $\Sigma_k$. Eigen-decompostion $\Sigma_k=U_kD_kU^T_k$, then the ingredients for $\delta_k$ are 
\begin{itemize}
	\item $(x-\hat{\mu_k})^T\hat{\Sigma}^{-1}(x-\hat{\mu_k})=[U^T_k(x-\hat{\mu_k})^T]D_k^{-1}[U^T_k(x-\hat{\mu_k})]$
	\item $log|\hat{\Sigma}_k|=\sum_{l}log d_{kl}$
\end{itemize}

3. For LDA, the new $X^*=D^{-1/2}U^TX$ so that $\hat{\Sigma}=UDU^T$.

4. popularity of LDA partially because it allows to view informative low-dim projection of the data. The K-centroids in p-dimensional input space lies in an affine subspace of dimension $\le K-1$.

\subsection*{7/17/2017 Ch4 P113- P119 }

1. \textcolor{red}{Last paragraph P113, how to understand "in locating the closest centroid, we can ignore distances orthogonal to this subspace, since they will contribute equally to each class."}

2. If class K=3, this allows us to view the data in a 2D plot; if $K > 3$, then we ask for a $L < K-1$ dimensional subspace optimal for LDA, where Fisher defined optimal to mean that the projected centroids were spread as much as possible in terms of variance(this amounts to finding principle component subspace of the centroids, maximizing the between class variance).

3. \textcolor{red}{how to get $B^*$ as the covariance matrix of $M^*= MW^{-1/2}$? i.e., the between class covariance. And why $v_l=W^{-1/2}v_l^*$? the l-th discriminant variable is $Z_l=v_l^TX$}

Between class variance is the variance of the class means of Z, and the within class variance is the pooled variance about the means. From Wikipedia, $$\Sigma_b = \frac{1}{C} \sum_{i=1}^C (\mu_i-\mu) (\mu_i-\mu)^T$$.The within-class variance is defined as $$\hat{\Sigma}=\sum_{k=1}^{K}\sum_{g_i=k}(x_i-\hat{\mu_k})(x_i-\hat{\mu_k})^T/(N-K)$$.

4. Fisher got the same decomposition by posing the problem: find the linear combination $Z=a^TX$ such that the between-class variance is maximized relative to the within-class variance. It amounts to maximizing the Rayleigh quotient,
\[ \displaystyle{\max_{a}}\frac{a^TBa}{a^TWa}
\]
or equivalently, 
\[ \displaystyle{\max_{a}}a^TBa \hspace{3mm}\text{s.t.} \hspace{3mm} a^TWa=1
\]
The optimal $a_1=v_1$ as defined above and next direction $a_2=v_2$, $a_l$ are referred to as discriminant coordinates or canonical variates

5. Summary in the end of Page 116 is very helpful! Since only the relative distances to the centroids count, one can confine the data to the subspace spanned by the centroids $H_{K-1}$ and make distance comparison there; The subspace can be further decomposed into successively optimal subspaces in term of \textcolor{red}{centroid separation}.


\subsection*{7/19/2017 Ch4 P117 - P122}

1. \textcolor{red}{how to understand the 2nd paragraph of page 117}

2. Fisher's reduced rank LDA amounts to the regression followed by an eigen-decomposition of $\hat{Y}^T\hat{Y}$.

3. In logistic regression,
$$Pr(G=k|X=x)=\frac{exp(\beta_{k0}+\beta_k^Tx)}{1+\sum_{l=1}^{K-1}exp(\beta_{l0}+\beta_l^Tx)}$$
$$Pr(G=K|X=x)=\frac{1}{1+\sum_{l=1}^{K-1}exp(\beta_{l0}+\beta_l^Tx)}$$
So the K-1 log-odds sum to 1.

4. $Pr(G|X)$ follows multinomial distribution, multi-class version of Binomial,  which has pmf $$P(X_1=x_1,X_2=x_2,\ldots,X_k=x_k)=\frac{n!}{x_1!x_2!\ldots x_k!}p_1^{x1}\ldots p_k^{xk}$$

5. Newton Raphson step for MLE can be re-expressed as a weighted least square problem(iteratively re-weighted least squares). R package glmnet can fit very large logistic regression problem efficiently for both large N and p.

6. logistics regression models are used mostly as a data analysis and inference tool, where the goal is to understand the role of input variables.

\subsection*{7/20/2017 Ch4 P122 - P127}

1. Heart disease example shows that variables, while on it own being significant, in the presence of many correlated variables, could be no longer significant and can even get opposite sign. Interpretation of coefficients in logistic regression: every unit of input increases accounts for an increase of exp(coef)*100\% in the response.

2. The connection between MLE of logistic regression and weighted least squares has offered convenience for software implementation to avoid iterative computations and take shortcut to test for inclusion of a term with Rao score test, exclusion of a term with Wald test. Rao's score test is for $H_0: \theta=\theta_0$, same for Wald test.

3. The $L_1$ penalty for Lasso can be used for variable selection and shrinkage with any linear regression model. For logistic regression, penalty is on its log-likelihood Equation(4.20).

4. Using the same quadratic approximation that were used in Newton algorithm, the penalized logistic regression can be solved by repeated application of weighted lasso algorithm.

5. Path algorithm such as LAR for lasso is difficult since the coefficient profiles are piecewise smooth rather than linear. R package glmpath used to generate coefficient path. Coordinate descent methods are also efficient to get profiles. 

\subsection*{7/21/2017 Ch4 P127 - P130}

1.logit transformation $log(p)=log(\frac{p}{1-p})$.

2. Writing $P(X,G)=P(X)P(G|X)$, for both logistic and LDA, the$P(G|X)$  has logit-linear form. Logistic regression only fit the P(G|X) by maximizing the conditional log-likelihood, while LDA maximizing the full log-likelihood $P(X,G)=\phi(X;\mu_k,\Sigma)\pi_k$, with $\phi$ being Gaussian density. Standard normal theory makes the parameter estimation more efficient(low variance). With $30\%$ more data, the conditional likelihood(logistic) will do as well.

3. LDA is not robust to gross(very large) outliers. It's generally felt that logistic regression is a safer, more robust than LDA model, relying on less assumption. It's our experience that the models give very similar results even when LDA is used inappropriately with qualitative predictors.

4. \textcolor{red}{why the signed distance from x to L is like (4.40)?}

\subsection*{7/24/2017 Ch7 Model Selection P219 - P224} 
1. Test error, also referred to a generalization error, is the prediction error over an independent test sample. For a given training set $\mathcal{T}$, the test error is 
$$Err_\mathcal{T}=E[Loss(Y,f(\hat{X}))|\mathcal{T}]$$

2. Training error is the average loss over the training samples $\bar{err}=\frac{1}{N}\sum_{i=1}^{N}L(Y_i, \hat{f}(x))$. Training error consistently decreases with model complexity, typically dropping to 0 if complex enough. As the model gets complex, there is a decrease in bias but an increase in variance.

3. Typical loss function for qualitative or categorical response:
$$L(G,\hat{G}(X))=I(G\ne \hat{G}(X))$$
$$L(G,\hat{G}(X))=-2\sum_{k=1}^{K}I(G=k)log\hat{p}_k(X)=-2log\hat{p}_G(X)$$
-2loglik is sometimes referred to as the deviance. The log-liklihood can be used as a loss function for general response densities. The -2 makes the loglik for Gaussian distribution match squared-error loss.

4. Ch7 describes methods for estimating the expected test error for a model, given tuning parameter $\alpha$, $\hat{f}_{\alpha}(X)$, and finding the $\alpha$ that minimizes error. i.e., model selection\\
Model assessment: having chosen a final model, estimating its prediction error on new data.

5. The training set is used to fit the models; the validation set is used to estimate the prediction error for model selection; the test set is used for assessment of the generalization error of the final chosen model. A typical split is 50,25,25. Validation step uses analytically methods(AIC, BIC, etc) or efficient sample re-use(cross-validation or bootstrap).

\end{document}
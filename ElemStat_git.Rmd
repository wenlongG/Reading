---
title: "The Elements of Statistical Learning"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Ch1 
6/16/2017

1. Supervised learning: having the presence of outcome variable to guide the learning process. Unsupervised learning:have no measurement of the outcome, our task is clustering.

2. Classification problem(predict categorical output)and regression problem(predict quantitative output)are Supervised learning;while clustering is Unsupervised learning.

##Ch2
6/18/2017

1. Linear model  makes huge assumption, yield stable but probably inaccurate predictions(low variance and high bias); while K-nearest neighbors model make mild assumptions, its predictions often accurate but unstable

2. P12,  1st paragraph, what is affine set? 	2nd paragraph, why steepest uphill direction?

3. The error on the training dataset should be approximately an increasing function of k for k-nearest neighbor fit; cannot use sum of squared error as a criteria for picking k.

4. The effective number of parameters in k-nearest neighbor is N/k, i.e., if non-overlapping, we need fit N/k means for totally N/k neighborhoods

5. If the training data is simulated from independent Gaussian distributions, least square is appropriate for prediction; if from mixture Gaussian, k-nearest neighbor is better.

6/19/2017

1. Bullets on P17,P18: kernel methods, local regression ... enhanced the simple 1(k)-nearest neighbor procedure. Kernel methods having weights decrease smoothly to zero with distance from the target point, rather than the 0/1 weights used by k-nearest neighbors.

2. Minimize the squared prediction error(EPE) loss, $EPE(f)= E_xE_{Y|X}([Y-f(X)]^2|X)$, get solution $f(x)= E(Y|X=x)$, i.e., the conditional expectation(known as the regression function). Nearest-neighbor approximates $\hat{f}(x)=Ave(y_i|x_i \in N_k(x))$. Under mild condition on P(X,Y), as N,k $\rightarrow \infty$ such that k/N $\rightarrow$ 0, $\hat{f}(x) \rightarrow E(Y|X=x)$. The convergence still holds but the rate of convergence decreases as the dimension p increases.

3. Additive model solves the high dimensional problem by approximating the universal conditional expectation simutaneously for each of the coordinate function $f_j$ in $f(X)=\sum_{j=1}^pf_j(X)$.

4. If $L_1$ loss: $E|Y-f(X)|$, the solution will be conditional median $f(x)= median(Y|X=x)$. The discontinuity in its derivatives hendered their wide use.

5. Loss function for categorical prediction is represented by a matrix L=card(G) with 0 on the diagonal and nonnegative elsewhere(often 1s). How to calculate (2.19) ~ (2.23)?



\documentclass[a4paper, 12pt]{article}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}

\usepackage[margin = 1in]{geometry}

\begin{document}
\section*{Chapter 2}


\textbf{Affine set} A subspace $S$ is a line or a plane go through the origin, i.e., $0\in S$. A subspace can be represented by a linear combination of basis vectors $b_k$'s
$$S = \{\lambda_1 b_1+\cdots+\lambda_p b_p, \lambda_k\in R\}$$.
If we move this subspace by an offset $o\notin S$ (or in the direction of $o$), we will get an affine set
$$A = o+S = \{o+\lambda_1 b_1+\cdots+\lambda_p b_p, \lambda_k\in R\}$$.

For what is discussed in the book, consider the simplest example with only one predictor
$$
y = \beta_0 + \beta_1 x
$$
\begin{itemize}
\item If the constant is included, then each point is $z = (1,x,y)^T$. Let $\gamma = (\beta_0, \beta_1, -1)$. Then the point $z$ is in the subspace defined by $S = \{z: z^T\gamma = 0\}$.
\item  If the constant is not included, then each point is $z = (x,y)$. Let $\gamma = (\beta_1, -1)$. Each point is in the affine set defined by $A = (0, \beta_0)^T + \{
z: z^T\gamma = 0\}$. The offset is $o=(0, \beta_0)^T$.
\end{itemize}

\vspace{30pt}
\noindent \textbf{Greatest uphill direction}. By first order Tylor expansion,
$$
f(X+\Delta) - f(X) \approx \langle f'(X) , \Delta \rangle = \Vert f'(x)\Vert \Vert \Delta\Vert \cos\theta
$$
where $\langle \va,\vb\rangle = \sum_i a_i b_i$ is the inner product of vector 
$\va$ and $\vb$. Inner product is equal to the vector norm mutiplied by $\cos\theta$, and $\theta$ is the angle between the two vectors. If $\Delta$ is in the same direction as $f'(x)=\beta$, then $\theta = 0$ and $\cos\theta = 1$. The increase of function value $f(X+\Delta) - f(x) $ is greatest.  

Image we are climbing a mountain, our current position is $X$ and the height is $f(X)$. We wish to go to the top of the mountain. 
If we walk from $X$ in the direction of $\beta$, we get greatest increase in $f(X)$ (uphill direction).


\vspace{30pt}
\noindent \textbf{Equation 2.19--2.23}. 
(2.19) takes expectation with respect to both $X,\ G$. 
$$
\mathrm{EPE} = E_{X,G}[L(G,\hat{G}(X)]
$$
(2.20) uses the following relation
$$
\mathrm{EPE} = E_X\Big[ E_G[L(G,\hat{G}(X) | X]\Big]
$$
The conditional expection inside is replace by the summation in
(2.20). Then (2.21) tries to minimize the above risk to get an
estimator $\hat{G}$. (2.22) and (2.23) are the solution for 0-1 loss
function. It classifies $X$ to the most probable class. 


\vspace{30pt}
\noindent \textbf{Equation 2.26, ($\hat{y}_0$)}. 

\begin{eqnarray*}
\hat{y}_0 &=& x_0^T\hat{\beta} = x_0^T (X^TX)^{-1} X^Ty\\
 & =& x_0^T (X^TX)^{-1} X^T (X\beta + \epsilon) \\
 & = & x_0^T (X^TX)^{-1} X^T X\beta +x_0 (X^TX)^{-1} X^T X \epsilon \\
 & = & x_0^T \beta +x_0 (X^TX)^{-1} X^T X \epsilon 
\end{eqnarray*}



\vspace{30pt}
\noindent \textbf{Section 2.6.1}
$Y = f(X) + \epsilon$. Only $\epsilon$ is random and its distribution
is known or fixed. The $f(X)$ shifts the mean of $\epsilon$. 
Given $X$, we know $f(X)$, so we also know the distribution of $Y$. 

\end{document}